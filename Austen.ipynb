{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in text from https://www.gutenberg.org/files/158/158-h/158-h.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import os\n",
    "import gensim\n",
    "import pickle\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_austen_text():\n",
    "    austen_text = \"\"\n",
    "    indir = \"/home/ubuntu/Notebooks/austenbooks\"\n",
    "    for root, dirs, filenames in os.walk(indir):\n",
    "        for filename in filenames:\n",
    "            if \".txt\" in filename:\n",
    "                path = indir+\"/\"+filename\n",
    "                f = open(path)\n",
    "                austen_text +=f.read()\n",
    "    return austen_text\n",
    "\n",
    "def get_text():\n",
    "    slate_text = \"\"\n",
    "    indir = '/home/ubuntu/Notebooks/OANC-GrAF/data/written_1/journal/slate'\n",
    "    directories = []\n",
    "    for root, dirs, filenames in os.walk(indir):\n",
    "        directories.append(dirs)\n",
    "    \n",
    "    \n",
    "    directories = directories[0]\n",
    "    for folder in directories:\n",
    "        indir = '/home/ubuntu/Notebooks/OANC-GrAF/data/written_1/journal/slate/'+folder\n",
    "        for root, dirs, filenames in os.walk(indir):\n",
    "            for filename in filenames:\n",
    "                if \".txt\" in filename:\n",
    "                    path = indir+\"/\"+filename\n",
    "                    f = open(path)\n",
    "                    slate_text +=f.read()\n",
    "    \n",
    "    \n",
    "    letters_text = \"\"\n",
    "    indir = \"/home/ubuntu/Notebooks/OANC-GrAF/data/written_1/letters/icic\"\n",
    "    for root, dirs, filenames in os.walk(indir):\n",
    "        for filename in filenames:\n",
    "            if \".txt\" in filename:\n",
    "                path = indir+\"/\"+filename\n",
    "                f = open(path)\n",
    "                letters_text +=f.read()\n",
    "                \n",
    "                \n",
    "    eggan_text = \"\"\n",
    "    path = \"/home/ubuntu/Notebooks/OANC-GrAF/data/written_1/fiction/eggan/TheStory.txt\"\n",
    "    f = open(path)\n",
    "    eggan_text +=f.read()\n",
    "    \n",
    "    \n",
    "    verbatim_text = \"\"\n",
    "    indir = \"/home/ubuntu/Notebooks/OANC-GrAF/data/written_1/journal/verbatim\"\n",
    "    for root, dirs, filenames in os.walk(indir):\n",
    "        for filename in filenames:\n",
    "            if \".txt\" in filename:\n",
    "                path = indir+\"/\"+filename\n",
    "                f = open(path)\n",
    "                verbatim_text +=f.read()\n",
    "                \n",
    "    \n",
    "    indir = '/home/ubuntu/Notebooks/OANC-GrAF/data/written_2/non-fiction/OUP'\n",
    "    directories = []\n",
    "    for root, dirs, filenames in os.walk(indir):\n",
    "        directories.append(dirs)\n",
    "    \n",
    "    directories = directories[0]\n",
    "    non_fiction_text = \"\"\n",
    "    for folder in directories:\n",
    "        indir = \"/home/ubuntu/Notebooks/OANC-GrAF/data/written_2/non-fiction/OUP/\"+folder\n",
    "        for root, dirs, filenames in os.walk(indir):\n",
    "            for filename in filenames:\n",
    "                if \".txt\" in filename:\n",
    "                    path = indir+\"/\"+filename\n",
    "                    f = open(path)\n",
    "                    non_fiction_text +=f.read()\n",
    "                    \n",
    "    \n",
    "    indir = '/home/ubuntu/Notebooks/OANC-GrAF/data/written_2/technical'\n",
    "    directories = []\n",
    "    for root, dirs, filenames in os.walk(indir):\n",
    "        directories.append(dirs)\n",
    "    \n",
    "    directories = directories[0]\n",
    "    technical_text = \"\"\n",
    "    for folder in directories:\n",
    "        indir = \"/home/ubuntu/Notebooks/OANC-GrAF/data/written_2/technical/\"+folder\n",
    "        for root, dirs, filenames in os.walk(indir):\n",
    "            for filename in filenames:\n",
    "                if \".txt\" in filename:\n",
    "                    path = indir+\"/\"+filename\n",
    "                    f = open(path)\n",
    "                    technical_text +=f.read()\n",
    "                    \n",
    "                    \n",
    "    gutenbooks = \"/home/ubuntu/Notebooks/gutenberg/blake-poems.txt\"\n",
    "    f=open(path)\n",
    "    gutenbooks += f.read()\n",
    "    indir = \"/home/ubuntu/Notebooks/gutenberg\"\n",
    "    for root, dirs, filenames in os.walk(indir):\n",
    "        for filename in filenames:\n",
    "            if (\".txt\" in filename) & (\"chesterton\" not in filename) & (\"shakespeare\" not in filename):\n",
    "                path = indir+\"/\"+filename\n",
    "                f = open(path)\n",
    "                gutenbooks +=f.read()\n",
    "                \n",
    "    \n",
    "\n",
    "    text = slate_text + letters_text + gutenbooks + technical_text + non_fiction_text + verbatim_text + eggan_text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "other_text = get_text()\n",
    "austen_text = get_austen_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "caps = \"([A-Z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    \n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"”\" in text: text = text.replace(\"”\",\"\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.lower()\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s.replace(\"“\", \"\") for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_text(target_text, other_text):\n",
    "    target_sentence_list = split_into_sentences(target_text)\n",
    "    other_sentence_list = split_into_sentences(other_text)\n",
    "    print('num of target, other sentences', len(target_sentence_list), len(other_sentence_list))\n",
    "    \n",
    "    word_tokenizer = TreebankWordTokenizer().tokenize\n",
    "    word_set = set()\n",
    "    for sentence in target_sentence_list+other_sentence_list:\n",
    "        word_set = word_set.union(set(word_tokenizer(sentence)))\n",
    "    print('number of words in vocab', len(word_set))\n",
    "    word_list = list(word_set)\n",
    "    num_to_word = {j:word for j, word in enumerate(word_list)}\n",
    "    word_to_num = {word:j for j, word in enumerate(word_list)}\n",
    "    \n",
    "    target_vectorized_sentences = np.array([np.array([word_to_num[word] for \\\n",
    "                                               word in word_tokenizer(sentence)]) \\\n",
    "                                            for sentence in target_sentence_list])\n",
    "    \n",
    "    other_vectorized_sentences = np.array([np.array([word_to_num[word] for \\\n",
    "                                               word in word_tokenizer(sentence)]) \\\n",
    "                                            for sentence in other_sentence_list])\n",
    "    success = True\n",
    "    return target_sentence_list, other_sentence_list, target_vectorized_sentences, other_vectorized_sentences, num_to_word, word_to_num, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of target, other sentences 35012 538166\n"
     ]
    }
   ],
   "source": [
    "target_sentence_list, other_sentence_list, target_vectorized_sentences, other_vectorized_sentences,\\\n",
    "    num_to_word, word_to_num, word_list = vectorize_text(austen_text, other_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('num_to_word.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump(num_to_word, picklefile)\n",
    "\n",
    "# with open('word_to_num.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump(word_to_num, picklefile)\n",
    "\n",
    "# with open('word_list.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump(word_list, picklefile)\n",
    "\n",
    "# with open('target_vectorized_sentences.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump(target_vectorized_sentences, picklefile)\n",
    "\n",
    "# with open('other_vectorized_sentences.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump(other_vectorized_sentences, picklefile)\n",
    "    \n",
    "# with open('target_sentence_list.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump(target_sentence_list, picklefile)\n",
    "\n",
    "# with open('other_sentence_list.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump(other_sentence_list, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_from_pickle():\n",
    "    with open(\"target_sentence_list.pkl\", 'rb') as picklefile: \n",
    "        target_sentence_list = pickle.load(picklefile)\n",
    "    with open(\"other_sentence_list.pkl\", 'rb') as picklefile: \n",
    "        other_sentence_list = pickle.load(picklefile)\n",
    "    with open(\"target_vectorized_sentences.pkl\", 'rb') as picklefile: \n",
    "        target_vectorized_sentences = pickle.load(picklefile)\n",
    "    with open(\"other_vectorized_sentences.pkl\", 'rb') as picklefile: \n",
    "        other_vectorized_sentences = pickle.load(picklefile)\n",
    "        \n",
    "    with open(\"word_to_num.pkl\", 'rb') as picklefile: \n",
    "        word_to_num = pickle.load(picklefile)\n",
    "    with open(\"word_list.pkl\", 'rb') as picklefile: \n",
    "        word_list = pickle.load(picklefile)\n",
    "    with open(\"num_to_word.pkl\", 'rb') as picklefile: \n",
    "        num_to_word = pickle.load(picklefile)\n",
    "        \n",
    "    return target_sentence_list, other_sentence_list, target_vectorized_sentences, other_vectorized_sentences,\\\n",
    "            num_to_word, word_to_num, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_sentence_list, other_sentence_list, target_vectorized_sentences, other_vectorized_sentences,\\\n",
    "            num_to_word, word_to_num, word_list = import_from_pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_subsets(vect):\n",
    "    train_indices = np.random.choice(len(vect), len(vect)//2, replace=False)\n",
    "    test_indices = [i for i in range(len(vect)) if i not in train_indices]\n",
    "    result_test = vect[test_indices]\n",
    "    result_train = vect[train_indices]\n",
    "    return result_test, result_train\n",
    "\n",
    "\n",
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "\n",
    "def make_test_train(targ_vect_sent, other_vect_sent):\n",
    "    targ_test, targ_train = make_subsets(targ_vect_sent)\n",
    "    other_test, other_train = make_subsets(other_vect_sent)\n",
    "    X_train = np.concatenate((targ_train, other_train))\n",
    "    X_test = np.concatenate((targ_test, other_test))\n",
    "    y_train = np.array([1]*len(targ_train)+ [0]*len(other_train))\n",
    "    y_test = np.array([1]*len(targ_test) + [0]*len(other_test))\n",
    "    X_train, y_train = shuffle_in_unison(X_train, y_train)\n",
    "        \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = make_test_train(target_vectorized_sentences, other_vectorized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def save_as_pickle():\n",
    "#     with open(\"X_train.pkl\", 'wb') as picklefile: \n",
    "#         pickle.dump(X_train, picklefile)\n",
    "#     with open(\"X_test.pkl\", 'wb') as picklefile: \n",
    "#         pickle.dump(X_test, picklefile)\n",
    "#     with open(\"y_train.pkl\", 'wb') as picklefile: \n",
    "#         pickle.dump(y_train, picklefile)\n",
    "#     with open(\"y_test.pkl\", 'wb') as picklefile: \n",
    "#         pickle.dump(y_test, picklefile)\n",
    "\n",
    "        \n",
    "# def import_from_pickle():\n",
    "#     with open(\"X_train.pkl\", 'rb') as picklefile: \n",
    "#         X_train = pickle.load(picklefile)\n",
    "#     with open(\"X_test.pkl\", 'rb') as picklefile: \n",
    "#         X_test = pickle.load(picklefile)\n",
    "#     with open(\"y_train.pkl\", 'rb') as picklefile: \n",
    "#         y_train = pickle.load(picklefile)\n",
    "#     with open(\"y_test.pkl\", 'rb') as picklefile: \n",
    "#         y_test = pickle.load(picklefile)\n",
    "        \n",
    "#     with open(\"word_to_num.pkl\", 'rb') as picklefile: \n",
    "#         word_to_num = pickle.load(picklefile)\n",
    "#     with open(\"word_list.pkl\", 'rb') as picklefile: \n",
    "#         word_list = pickle.load(picklefile)\n",
    "#     with open(\"num_to_word.pkl\", 'rb') as picklefile: \n",
    "#         num_to_word = pickle.load(picklefile)\n",
    "        \n",
    "#     return X_train, X_test, y_train, y_test, num_to_word, word_to_num, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test, num_to_word, word_to_num, word_list = import_from_pickle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_review_length = 50\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 32)            5424608   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 5,507,169.0\n",
      "Trainable params: 5,507,169\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/keras/models.py:826: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 286589 samples, validate on 286589 samples\n",
      "Epoch 1/2\n",
      "286589/286589 [==============================] - 1370s - loss: 0.0768 - acc: 0.9721 - val_loss: 0.0517 - val_acc: 0.9793\n",
      "Epoch 2/2\n",
      "286589/286589 [==============================] - 1522s - loss: 0.0401 - acc: 0.9840 - val_loss: 0.0475 - val_acc: 0.9808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f83926525c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_list), embedding_vector_length, input_length=max_review_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.save('model2epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model2 = load_model('model2epoch.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect model for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_prob_on_sentence(sentence, model):\n",
    "    word_tokenizer = TreebankWordTokenizer().tokenize\n",
    "    vec_sentence = np.array([np.array([word_to_num[word] for \\\n",
    "                                               word in word_tokenizer(sentence)])])\n",
    "    padded_sentence = sequence.pad_sequences(vec_sentence, maxlen=50)\n",
    "    return model.predict_proba(padded_sentence)\n",
    "\n",
    "def predict_prob_on_paragraph(paragraph, model):\n",
    "    sentence_list = split_into_sentences(paragraph)\n",
    "    results = []\n",
    "    for sentence in sentence_list:\n",
    "        results.append(predict_prob_on_sentence(sentence, model)[0][0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "[[ 0.91311961]] [[ 0.96459585]]\n"
     ]
    }
   ],
   "source": [
    "s = 'she was the youngest of the two daughters of a most affectionate, indulgent father'\n",
    "t= 'the event had every promise of happiness for her friend.'\n",
    "print(predict_prob_on_sentence(s, model2), predict_prob_on_sentence(t,model2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_doc_2_vec(list_of_sentences, size=100, min_count=5, dm=1, iter=1, workers=4, dbow_words=1):\n",
    "    '''Text should be a list of sentence strings. Returns doc2vec model and list of tagged sentences'''\n",
    "    list_of_tagged_sentences = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i]) \\\n",
    "          for i, line in enumerate(list_of_sentences)]\n",
    "    docmodel = gensim.models.doc2vec.Doc2Vec(size=size, min_count=min_count, dm=dm, iter=iter, workers=workers, dbow_words=dbow_words)\n",
    "    docmodel.build_vocab(list_of_tagged_sentences)\n",
    "    docmodel.train(list_of_tagged_sentences)\n",
    "    return list_of_tagged_sentences, docmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_sentences = target_sentence_list + other_sentence_list\n",
    "list_of_tagged_sentences = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i]) \\\n",
    "          for i, line in enumerate(combined_sentences)]\n",
    "docmodel = gensim.models.doc2vec.Doc2Vec(size=100, min_count=5, dm=1, iter=100, workers=16, dbow_words=1)\n",
    "docmodel.build_vocab(list_of_tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "573178"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docmodel.train(list_of_tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.18405372]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = docmodel.infer_vector(list_of_tagged_sentences[28873].words)\n",
    "v2 = docmodel.infer_vector(list_of_tagged_sentences[27333].words)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(299853, 0.57945716381073), (309920, 0.558485746383667), (294920, 0.5492386817932129), (24318, 0.5309747457504272), (25083, 0.5235756635665894), (552629, 0.5155673027038574), (292002, 0.5140426158905029), (182256, 0.5090835094451904), (307558, 0.5075476765632629), (23924, 0.501731276512146)]\n",
      "oh, go, go!\n",
      "\n",
      "\n",
      "then solomon sent benaiah the son of jehoiada, saying, go, fall upon him.\n",
      "\n",
      "\n",
      "i say run!\n",
      "\n",
      "\n",
      "go on, go on.\n",
      "\n",
      "\n",
      "but go on, go on.\n",
      "\n",
      "\n",
      "i say tomato, you say tomahto.\n",
      "\n",
      "\n",
      "come at six o'clock and go at six?\n",
      "\n",
      "\n",
      "walk the walk and talk the talk.\n",
      "\n",
      "\n",
      "11:38 and he said, go.\n",
      "\n",
      "\n",
      "fanny, you do not want to go, do you?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inferred_vector = docmodel.infer_vector(['i', 'say', 'we', 'go', 'for', 'a', 'walk', 'this', 'afternoon'])\n",
    "sims = docmodel.docvecs.most_similar([inferred_vector], topn=10)\n",
    "print(sims)\n",
    "\n",
    "for docid in [docid for docid, sims in sims]:\n",
    "    print(combined_sentences[docid])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(325278, 0.584077000617981), (523253, 0.5687791109085083), (299406, 0.5549490451812744), (299853, 0.5469987988471985), (531553, 0.5449169874191284), (2144, 0.5293335318565369), (409651, 0.5240321159362793), (9681, 0.5207614898681641), (156439, 0.5202246904373169), (24318, 0.5169010162353516)]\n",
      "go and see.\n",
      "\n",
      "\n",
      "go for lunch, and dinner, and come back the next day.\n",
      "\n",
      "\n",
      "go, my dear; go and dance again.\n",
      "\n",
      "\n",
      "oh, go, go!\n",
      "\n",
      "\n",
      "), solving a dicult puzzle (where does this piece go?\n",
      "\n",
      "\n",
      "but i dare say you will go for all that.\n",
      "\n",
      "\n",
      "5%) to induce growth arrest in go, the levels of         gsk3β activity were increased compared to asynchronously         growing cells (fig 4, go).\n",
      "\n",
      "\n",
      "stay, stay, i will go myself.\n",
      "\n",
      "\n",
      "why, there's no need even to go outside and collect the newspaper.\n",
      "\n",
      "\n",
      "go on, go on.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inferred_vector = docmodel.infer_vector(['where', 'do', 'you', 'want', 'to', 'go', 'for', 'lunch'])\n",
    "sims = docmodel.docvecs.most_similar([inferred_vector], topn=10)\n",
    "print(sims)\n",
    "\n",
    "for docid in [docid for docid, sims in sims]:\n",
    "    print(combined_sentences[docid])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(283098, 0.6869240403175354), (272756, 0.6668916940689087), (336324, 0.6573529839515686), (280888, 0.6559344530105591), (569856, 0.6473436951637268), (338266, 0.644770622253418), (342868, 0.6348352432250977), (113143, 0.6308819055557251), (557788, 0.6183530688285828), (526547, 0.6179397106170654)]\n",
      "you, daggoo!\n",
      "\n",
      "\n",
      "last year, you generously       donated $ .\n",
      "\n",
      "\n",
      "you arches!\n",
      "\n",
      "\n",
      "over descartian vortices you hover.\n",
      "\n",
      "\n",
      "are you a musician?\n",
      "\n",
      "\n",
      "of you gray rocks!\n",
      "\n",
      "\n",
      ", 1994; you, et al.\n",
      "\n",
      "\n",
      "bad tempered and bad for you, says the new                 yorker 's anthony lane.\n",
      "\n",
      "\n",
      ")  you cursed brat!\n",
      "\n",
      "\n",
      "you and i have advantages of trade.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inferred_vector = docmodel.infer_vector(['i', 'love', 'you'])\n",
    "sims = docmodel.docvecs.most_similar([inferred_vector], topn=10)\n",
    "print(sims)\n",
    "\n",
    "for docid in [docid for docid, sims in sims]:\n",
    "    print(combined_sentences[docid])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docmodel.save('docmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "loadeddocmodel = doc2vec.Doc2Vec.load('docmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120245845"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sentences = target_sentence_list + other_sentence_list\n",
    "list_of_tagged_sentences = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i]) \\\n",
    "          for i, line in enumerate(combined_sentences)]\n",
    "docmodel3 = gensim.models.doc2vec.Doc2Vec(size=100, min_count=10, iter=30, sample=0.00001, \\\n",
    "                                         workers=16, dm=0, dbow_words=1, seed=0)\n",
    "docmodel3.build_vocab(list_of_tagged_sentences)\n",
    "\n",
    "len(list_of_tagged_sentences)\n",
    "\n",
    "docmodel3.train(list_of_tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(179026, 0.819800853729248), (307774, 0.8137213587760925), (54269, 0.8100277185440063), (75487, 0.8056532740592957), (14109, 0.8017879724502563), (72970, 0.8014056086540222), (15435, 0.7987525463104248), (16529, 0.7956112027168274), (299042, 0.7949422597885132), (205454, 0.7938140034675598)]\n",
      "perhaps a subject for this afternoon?\n",
      "\n",
      "\n",
      "and they tarried until afternoon, and they did eat both of them.\n",
      "\n",
      "\n",
      "a few more like this morning's and he'll be kaput.\n",
      "\n",
      "\n",
      "then, on any given night, you'd run into twice as many of them.\n",
      "\n",
      "\n",
      "it is a very cold afternoon—but in this carriage we know nothing of the matter.\n",
      "\n",
      "\n",
      "if you don't want it, you can dispose of it this afternoon.\n",
      "\n",
      "\n",
      "i think it would be much better if they would come in one afternoon next summer, and take their tea with us—take us in their afternoon walk; which they might do, as our hours are so reasonable, and yet get home without being out in the damp of the evening.\n",
      "\n",
      "\n",
      "and you must be off this very morning?\n",
      "\n",
      "\n",
      "who sent us breakfast?\n",
      "\n",
      "\n",
      "many of our friends are relieved, i think, not to have to give up a summer sunday afternoon.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inferred_vector = docmodel3.infer_vector(['i', 'say', 'we', 'go', 'for', 'a', 'walk', 'this', 'afternoon'])\n",
    "sims = docmodel3.docvecs.most_similar([inferred_vector], topn=10)\n",
    "print(sims)\n",
    "\n",
    "for docid in [docid for docid, sims in sims]:\n",
    "    print(combined_sentences[docid])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(294401, 0.7325917482376099), (34806, 0.7310218214988708), (28906, 0.7276325225830078), (24008, 0.718917965888977), (18470, 0.7148057818412781), (18759, 0.7122348546981812), (19693, 0.7080596685409546), (38518, 0.7055720090866089), (21800, 0.6979830861091614), (287924, 0.6972124576568604)]\n",
      "may i call again, madam, this afternoon?\n",
      "\n",
      "\n",
      "she must not walk.\n",
      "\n",
      "\n",
      "walk!\n",
      "\n",
      "\n",
      "walk!\n",
      "\n",
      "\n",
      "—should not they walk?\n",
      "\n",
      "\n",
      "shall we walk, augusta?\n",
      "\n",
      "\n",
      "—only that morning.\n",
      "\n",
      "\n",
      "in the morning .\n",
      "\n",
      "\n",
      "but any morning will do for this.\n",
      "\n",
      "\n",
      "is't night?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_sentences = target_sentence_list + other_sentence_list\n",
    "list_of_tagged_sentences = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i]) \\\n",
    "          for i, line in enumerate(combined_sentences)]\n",
    "docmodel4 = gensim.models.doc2vec.Doc2Vec(size=100, min_count=10, iter=30, sample=0.0001, \\\n",
    "                                         workers=16, dm=0, dbow_words=1, seed=0)\n",
    "docmodel4.build_vocab(list_of_tagged_sentences)\n",
    "\n",
    "len(list_of_tagged_sentences)\n",
    "\n",
    "docmodel4.train(list_of_tagged_sentences)\n",
    "\n",
    "inferred_vector = docmodel4.infer_vector(['i', 'say', 'we', 'go', 'for', 'a', 'walk', 'this', 'afternoon'])\n",
    "sims = docmodel4.docvecs.most_similar([inferred_vector], topn=10)\n",
    "print(sims)\n",
    "\n",
    "for docid in [docid for docid, sims in sims]:\n",
    "    print(combined_sentences[docid])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_sentences = target_sentence_list + other_sentence_list\n",
    "list_of_tagged_sentences = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i]) \\\n",
    "          for i, line in enumerate(combined_sentences)]\n",
    "docmodel5 = gensim.models.doc2vec.Doc2Vec(size=100, min_count=10, iter=30, sample=0.000001, \\\n",
    "                                         workers=16, dm=0, dbow_words=1, seed=0)\n",
    "docmodel5.build_vocab(list_of_tagged_sentences)\n",
    "\n",
    "len(list_of_tagged_sentences)\n",
    "\n",
    "docmodel5.train(list_of_tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(132764, 0.9279472827911377), (523039, 0.9273142218589783), (231727, 0.927098274230957), (209908, 0.9268016815185547), (238081, 0.9267863035202026), (76230, 0.9253809452056885), (68485, 0.9251948595046997), (235863, 0.9245360493659973), (29689, 0.9236310124397278), (89388, 0.9235737323760986)]\n",
      "but i wonder if the scientists behind this important philosophical shift have really considered all the consequences of their actions.\n",
      "\n",
      "\n",
      "i do think my claim is true.\n",
      "\n",
      "\n",
      "as for something possibly underlying seemingly trivial complaints, there need not necessarily be a repressed anything.\n",
      "\n",
      "\n",
      "he thinks that these movies are destructive, that they enjoy degradation.\n",
      "\n",
      "\n",
      "why don't you help me a little, he replied, and not ask me to define the word 'permissive.\n",
      "\n",
      "\n",
      "getting the police to respond has been useless and seems petty anyway.\n",
      "\n",
      "\n",
      "but it doesn't explain why the reverse couldn't turn out to be true instead.\n",
      "\n",
      "\n",
      "that pretty much expresses prudie's thoughts on the matter.\n",
      "\n",
      "\n",
      "i cannot think how it could happen!\n",
      "\n",
      "\n",
      "if you're looking for a firm conclusion to all this, you'll have to look elsewhere; i hope i've at least illuminated some of the attendant moral and economic issues  though even these can become very different in situations that are superficially similar.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inferred_vector = docmodel5.infer_vector(['i', 'say', 'we', 'go', 'for', 'a', 'walk', 'this', 'afternoon'])\n",
    "sims = docmodel5.docvecs.most_similar([inferred_vector], topn=10)\n",
    "print(sims)\n",
    "\n",
    "for docid in [docid for docid, sims in sims]:\n",
    "    print(combined_sentences[docid])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(179026, 0.7787976264953613), (171542, 0.7507870197296143), (29575, 0.7299491167068481), (26806, 0.7297888398170471), (1025, 0.7149198055267334), (63658, 0.7112331986427307), (66809, 0.7084373235702515), (17137, 0.7082470059394836), (4562, 0.7049012184143066), (14980, 0.7034071683883667)]\n",
      "perhaps a subject for this afternoon?\n",
      "\n",
      "\n",
      "the weekly e mail delivery should arrive friday afternoon as well.\n",
      "\n",
      "\n",
      "good morning to you.\n",
      "\n",
      "\n",
      "i did not arrive here till tuesday evening.\n",
      "\n",
      "\n",
      "—and are you going this morning?\n",
      "\n",
      "\n",
      "on the afternoon of oct.\n",
      "\n",
      "\n",
      "5, 1998)                                           evening.\n",
      "\n",
      "\n",
      "a walk before breakfast does me good.\n",
      "\n",
      "\n",
      "— such an evening!\n",
      "\n",
      "\n",
      "—good morning to you.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_sentences = target_sentence_list + other_sentence_list\n",
    "list_of_tagged_sentences = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i]) \\\n",
    "          for i, line in enumerate(combined_sentences)]\n",
    "docmodel6 = gensim.models.doc2vec.Doc2Vec(size=100, min_count=10, iter=30, sample=0.00005, \\\n",
    "                                         workers=16, dm=0, dbow_words=1, seed=0)\n",
    "docmodel6.build_vocab(list_of_tagged_sentences)\n",
    "docmodel6.train(list_of_tagged_sentences)\n",
    "\n",
    "inferred_vector = docmodel6.infer_vector(['i', 'say', 'we', 'go', 'for', 'a', 'walk', 'this', 'afternoon'])\n",
    "sims = docmodel6.docvecs.most_similar([inferred_vector], topn=10)\n",
    "print(sims)\n",
    "\n",
    "for docid in [docid for docid, sims in sims]:\n",
    "    print(combined_sentences[docid])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(295613, 0.7797182202339172), (72970, 0.7774734497070312), (552644, 0.7773740291595459), (25865, 0.7767965793609619), (18759, 0.7745453119277954), (318626, 0.7727531790733337), (15516, 0.7718029022216797), (304898, 0.7707496881484985), (33577, 0.7701761722564697), (17137, 0.7686857581138611)]\n",
      "i think i could walk round him.\n",
      "\n",
      "\n",
      "if you don't want it, you can dispose of it this afternoon.\n",
      "\n",
      "\n",
      "we'll get a look at uranus tomorrow!\n",
      "\n",
      "\n",
      "i am come to walk with you, fanny, said he.\n",
      "\n",
      "\n",
      "shall we walk, augusta?\n",
      "\n",
      "\n",
      "2:5 o house of jacob, come ye, and let us walk in the light of the lord.\n",
      "\n",
      "\n",
      "now i shall really be very happy to walk into the same room with you.\n",
      "\n",
      "\n",
      "26:12 and i will walk among you, and will be your god, and ye shall be my people.\n",
      "\n",
      "\n",
      "yes, yes we will have a snug walk together, and i have something to tell you as we go along.\n",
      "\n",
      "\n",
      "a walk before breakfast does me good.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_sentences = target_sentence_list + other_sentence_list\n",
    "list_of_tagged_sentences = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i]) \\\n",
    "          for i, line in enumerate(combined_sentences)]\n",
    "docmodel7 = gensim.models.doc2vec.Doc2Vec(size=100, min_count=10, iter=30, sample=0.000025, \\\n",
    "                                         workers=16, dm=0, dbow_words=1, seed=0)\n",
    "docmodel7.build_vocab(list_of_tagged_sentences)\n",
    "docmodel7.train(list_of_tagged_sentences)\n",
    "\n",
    "inferred_vector = docmodel7.infer_vector(['i', 'say', 'we', 'go', 'for', 'a', 'walk', 'this', 'afternoon'])\n",
    "sims = docmodel7.docvecs.most_similar([inferred_vector], topn=10)\n",
    "print(sims)\n",
    "\n",
    "for docid in [docid for docid, sims in sims]:\n",
    "    print(combined_sentences[docid])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLDER STUFF. CAN IGNORE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docmodel.save('docmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.7134791]], dtype=float32)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model.infer_vector(list_of_tagged_sentences[28873].words)\n",
    "v2 = model.infer_vector(list_of_tagged_sentences[27333].words)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.88064344,  0.09701519])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model.infer_vector(['i', 'say', 'we', 'go', 'for', 'a', 'walk', 'this', 'afternoon'])\n",
    "v2 = model.infer_vector(list_of_tagged_sentences[845].words)\n",
    "\n",
    "cosine_similarity(v1,v2)\n",
    "model.similarity(['at', 'the'], 'at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "does she go to balls?\n",
      "[(14931, 0.578771710395813), (17043, 0.5734875798225403), (23826, 0.5693657994270325), (29419, 0.5685966610908508), (34358, 0.5607420206069946), (2144, 0.559876561164856), (845, 0.5560895800590515), (26631, 0.5557641983032227), (27927, 0.5550302863121033), (18996, 0.5496828556060791)]\n"
     ]
    }
   ],
   "source": [
    "inferred_vector = model.infer_vector(['i', 'say', 'we', 'go', 'for', 'a', 'walk', 'this', 'afternoon'])\n",
    "print(austen_sentences[21514])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=10)\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inferred_vector2 = model.infer_vector(['elizabeth','was','forced','to','go'])\n",
    "inferred_vector3 = model.infer_vector(['go','and','interfere'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5,100) and (3,100) not aligned: 100 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-362-737bc941b765>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'elizabeth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'was'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'forced'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'to'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'go'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'go'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'and'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'interfere'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \"\"\"\n\u001b[0;32m--> 594\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (5,100) and (3,100) not aligned: 100 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "model.similarity(['elizabeth','was','forced','to','go'],['go','and','interfere'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(29557, 0.5815350413322449), (28545, 0.5791788697242737), (15817, 0.5709363222122192), (2144, 0.5642344951629639), (34358, 0.563313901424408), (14303, 0.5623300075531006), (21514, 0.5601113438606262), (26855, 0.5595582723617554), (18996, 0.5546441078186035), (845, 0.5527011156082153)]\n",
      "’ i say, you will come to belle’s wedding, i hope.\n",
      "\n",
      "i would not be bound to go two miles in it for fifty thousand pounds.\n",
      "\n",
      "go, and interfere.\n",
      "\n",
      "but i dare say you will go for all that.\n",
      "\n",
      "indeed, i do say it.\n",
      "\n",
      "she tried to stop him; but vainly; he would go on, and say it all.\n",
      "\n",
      "does she go to balls?\n",
      "\n",
      "do not say so.\n",
      "\n",
      "—‘you must go,’ said she.\n",
      "\n",
      "we must go, said sir john.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inferred_vector = model.infer_vector(['i', 'say', 'we', 'go', 'for', 'a', 'walk', 'this', 'afternoon'])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=10)\n",
    "print(sims)\n",
    "\n",
    "for docid in [docid for docid, sims in sims]:\n",
    "    print(austen_sentences[docid])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "austen_sentences = split_into_sentences(austen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i]) \\\n",
    "          for i, line in enumerate(austen_sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['the', 'family', 'of', 'dashwood', 'had', 'long', 'been', 'settled', 'in', 'sussex'], tags=[0])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(size=50, min_count=2, iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.build_vocab(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27751403"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Should get rid of \" -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(24876, 0.731322705745697), (21756, 0.7059229016304016), (33822, 0.6980264186859131), (29579, 0.6944616436958313), (17172, 0.6897308230400085), (33624, 0.6850894689559937), (28119, 0.6787413954734802), (16466, 0.6751720309257507), (32443, 0.674387514591217), (33181, 0.673367977142334)]\n",
      "it was a heavy, melancholy day.\n",
      "\n",
      "the second day’s trial was not so guiltless.\n",
      "\n",
      "about a fortnight.\n",
      "\n",
      "and i hope—i hope, miss morland, you will not be sorry to see me.\n",
      "\n",
      "you do us a great deal of honour to day, i am sure.\n",
      "\n",
      "we have seen nothing of him since november.\n",
      "\n",
      "i am sure you would be miserable if you thought so!\n",
      "\n",
      "miss woodhouse, i hope nothing may happen to prevent the ball.\n",
      "\n",
      "i wish she had accepted him.\n",
      "\n",
      "as yet, you have seen nothing of bath.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inferred_vector = model.infer_vector(['i', 'hope', 'it', 'is', 'a', 'glorious', 'day', 'today'])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=10)\n",
    "print(sims)\n",
    "\n",
    "for docid in [docid for docid, sims in sims]:\n",
    "    print(austen_sentences[docid])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0176868 , -0.00257066,  0.00481869,  0.02462567, -0.02775163,\n",
       "        0.04307807, -0.00611855, -0.00384286,  0.00371891, -0.02889272,\n",
       "       -0.00826401,  0.00873966,  0.01128767,  0.0047045 ,  0.00025823,\n",
       "       -0.05126639, -0.04118882,  0.02682292,  0.04621991, -0.06935307,\n",
       "        0.01737165, -0.00050234,  0.02227636, -0.00110546,  0.00068362,\n",
       "       -0.01858515, -0.03066764,  0.0093579 ,  0.04157468,  0.02754266,\n",
       "       -0.02184875, -0.0104305 , -0.04991533,  0.00858509, -0.00390728,\n",
       "       -0.04692287,  0.02847849,  0.02653959, -0.00522324,  0.03872556,\n",
       "       -0.00789808,  0.05382846, -0.02034015, -0.0190276 ,  0.02660172,\n",
       "       -0.01940051,  0.01538533, -0.01962281, -0.0076128 ,  0.01571602], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inferred_vector = model.infer_vector(['are', 'you', 'quite', 'sure', 'of', 'it', '?'])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(31557, 0.8228976726531982),\n",
       " (32794, 0.8210653066635132),\n",
       " (11291, 0.8179731369018555),\n",
       " (29305, 0.8168315887451172),\n",
       " (11365, 0.8157533407211304),\n",
       " (21369, 0.8155891299247742),\n",
       " (4559, 0.810936689376831),\n",
       " (12018, 0.8092895746231079),\n",
       " (14979, 0.8092561960220337),\n",
       " (21348, 0.8089516162872314),\n",
       " (26539, 0.8086695671081543),\n",
       " (11337, 0.8086438179016113),\n",
       " (9844, 0.8069813847541809),\n",
       " (823, 0.8065879344940186),\n",
       " (15236, 0.8055530190467834),\n",
       " (11032, 0.8055527210235596),\n",
       " (4498, 0.8011966943740845),\n",
       " (6704, 0.800519585609436),\n",
       " (31856, 0.7996350526809692),\n",
       " (28940, 0.7994403839111328),\n",
       " (8574, 0.7992561459541321),\n",
       " (29788, 0.7990227341651917),\n",
       " (19422, 0.7986496090888977),\n",
       " (11202, 0.7971107959747314),\n",
       " (19608, 0.7967904210090637),\n",
       " (29647, 0.7961963415145874),\n",
       " (16871, 0.7958846688270569),\n",
       " (11239, 0.7943636775016785),\n",
       " (2299, 0.7940699458122253),\n",
       " (3546, 0.7935033440589905),\n",
       " (1276, 0.7934448719024658),\n",
       " (25076, 0.793043851852417),\n",
       " (11265, 0.7929806709289551),\n",
       " (3544, 0.7929611206054688),\n",
       " (27248, 0.7928163409233093),\n",
       " (12056, 0.7926017642021179),\n",
       " (29296, 0.7921662330627441),\n",
       " (16357, 0.7921057343482971),\n",
       " (2174, 0.791382372379303),\n",
       " (28543, 0.791255533695221),\n",
       " (33137, 0.7908014059066772),\n",
       " (28861, 0.7907714247703552),\n",
       " (24226, 0.7903364896774292),\n",
       " (18809, 0.7898232340812683),\n",
       " (31000, 0.7893479466438293),\n",
       " (32623, 0.7888997793197632),\n",
       " (25934, 0.7885602712631226),\n",
       " (2580, 0.7880059480667114),\n",
       " (15700, 0.7880026698112488),\n",
       " (34547, 0.7876629829406738),\n",
       " (13357, 0.7876492142677307),\n",
       " (1935, 0.787056565284729),\n",
       " (21595, 0.7868949174880981),\n",
       " (16358, 0.7867168188095093),\n",
       " (31056, 0.7857331037521362),\n",
       " (14328, 0.785653293132782),\n",
       " (4089, 0.7854158878326416),\n",
       " (6458, 0.7853555679321289),\n",
       " (24317, 0.7853045463562012),\n",
       " (2389, 0.7852452397346497),\n",
       " (34245, 0.7852083444595337),\n",
       " (24671, 0.7850921154022217),\n",
       " (11035, 0.7849297523498535),\n",
       " (26254, 0.784717321395874),\n",
       " (19909, 0.7842074632644653),\n",
       " (4669, 0.7841604948043823),\n",
       " (11462, 0.7841277122497559),\n",
       " (11954, 0.7838426232337952),\n",
       " (19145, 0.7833753824234009),\n",
       " (28881, 0.7832896113395691),\n",
       " (15265, 0.7831138372421265),\n",
       " (7485, 0.7830485105514526),\n",
       " (17358, 0.7829009294509888),\n",
       " (17512, 0.7826099395751953),\n",
       " (1573, 0.7821616530418396),\n",
       " (19134, 0.7820397019386292),\n",
       " (16533, 0.7819779515266418),\n",
       " (4208, 0.7819651961326599),\n",
       " (14994, 0.7818619012832642),\n",
       " (24921, 0.7816590666770935),\n",
       " (32636, 0.7815381288528442),\n",
       " (25299, 0.7813847064971924),\n",
       " (12599, 0.7813658714294434),\n",
       " (33494, 0.781234860420227),\n",
       " (17784, 0.7811936140060425),\n",
       " (20595, 0.7811373472213745),\n",
       " (26114, 0.780972957611084),\n",
       " (19725, 0.7806020975112915),\n",
       " (4197, 0.7804892659187317),\n",
       " (11654, 0.7803089618682861),\n",
       " (31761, 0.7800917625427246),\n",
       " (18784, 0.7797794342041016),\n",
       " (15650, 0.7796913385391235),\n",
       " (15563, 0.7796691656112671),\n",
       " (4200, 0.7795649766921997),\n",
       " (21708, 0.7794095277786255),\n",
       " (11366, 0.7793127298355103),\n",
       " (24139, 0.7790616750717163),\n",
       " (31539, 0.7788748741149902),\n",
       " (9667, 0.7788066864013672),\n",
       " (3952, 0.7787731885910034),\n",
       " (33366, 0.7787570953369141),\n",
       " (19119, 0.7785439491271973),\n",
       " (32612, 0.7784154415130615),\n",
       " (15761, 0.7783925533294678),\n",
       " (16137, 0.7775862216949463),\n",
       " (18902, 0.7775488495826721),\n",
       " (108, 0.7774412035942078),\n",
       " (16561, 0.7774106860160828),\n",
       " (4646, 0.7772525548934937),\n",
       " (28211, 0.7771488428115845),\n",
       " (24329, 0.7770658731460571),\n",
       " (26231, 0.7768728733062744),\n",
       " (18080, 0.7766321897506714),\n",
       " (20667, 0.776502251625061),\n",
       " (7408, 0.7764878273010254),\n",
       " (17935, 0.7763717770576477),\n",
       " (28218, 0.7763243913650513),\n",
       " (11872, 0.7762527465820312),\n",
       " (7755, 0.7760738730430603),\n",
       " (12933, 0.776019811630249),\n",
       " (29672, 0.7757312655448914),\n",
       " (19621, 0.7757298946380615),\n",
       " (12505, 0.7757062911987305),\n",
       " (17673, 0.7756407260894775),\n",
       " (34750, 0.7753990888595581),\n",
       " (4745, 0.7753708362579346),\n",
       " (18592, 0.7753655910491943),\n",
       " (11493, 0.7753374576568604),\n",
       " (2717, 0.7752596735954285),\n",
       " (12069, 0.7748757004737854),\n",
       " (32200, 0.7747150659561157),\n",
       " (30238, 0.774700403213501),\n",
       " (1439, 0.7745424509048462),\n",
       " (24735, 0.7745194435119629),\n",
       " (9662, 0.7743521928787231),\n",
       " (28104, 0.7742120027542114),\n",
       " (1914, 0.7741806507110596),\n",
       " (16136, 0.7741451859474182),\n",
       " (563, 0.7740874886512756),\n",
       " (80, 0.7739377021789551),\n",
       " (12082, 0.7737240791320801),\n",
       " (1730, 0.7737120389938354),\n",
       " (4663, 0.7736327052116394),\n",
       " (11212, 0.7735511660575867),\n",
       " (21940, 0.7733435034751892),\n",
       " (11306, 0.7731457948684692),\n",
       " (33347, 0.7731356024742126),\n",
       " (9678, 0.7731336355209351),\n",
       " (29503, 0.7730870246887207),\n",
       " (572, 0.7730188369750977),\n",
       " (15127, 0.7728986740112305),\n",
       " (31766, 0.7727757692337036),\n",
       " (5425, 0.7726736068725586),\n",
       " (2368, 0.7725304961204529),\n",
       " (9150, 0.7724878787994385),\n",
       " (24332, 0.7723385691642761),\n",
       " (20543, 0.7723292112350464),\n",
       " (1706, 0.7722587585449219),\n",
       " (24650, 0.7721214294433594),\n",
       " (15327, 0.7720638513565063),\n",
       " (2654, 0.7719442844390869),\n",
       " (26592, 0.771843433380127),\n",
       " (1893, 0.7717708349227905),\n",
       " (2870, 0.7716472148895264),\n",
       " (3084, 0.7716097831726074),\n",
       " (26998, 0.7715386152267456),\n",
       " (24985, 0.771379828453064),\n",
       " (15481, 0.7711906433105469),\n",
       " (18045, 0.7710404992103577),\n",
       " (29965, 0.7709333300590515),\n",
       " (30693, 0.7708151340484619),\n",
       " (14940, 0.7706487774848938),\n",
       " (6197, 0.7705851197242737),\n",
       " (26102, 0.7704378366470337),\n",
       " (14995, 0.7704302072525024),\n",
       " (31569, 0.7702779769897461),\n",
       " (1223, 0.7702743411064148),\n",
       " (29163, 0.770168125629425),\n",
       " (9572, 0.7701173424720764),\n",
       " (19315, 0.7701021432876587),\n",
       " (33587, 0.7699517607688904),\n",
       " (21218, 0.7699132561683655),\n",
       " (31508, 0.7697824239730835),\n",
       " (8512, 0.7693111896514893),\n",
       " (26645, 0.7690070867538452),\n",
       " (28638, 0.7689298391342163),\n",
       " (3638, 0.7687801718711853),\n",
       " (27340, 0.7685821056365967),\n",
       " (11213, 0.7685602903366089),\n",
       " (6065, 0.7684673070907593),\n",
       " (10473, 0.7684313058853149),\n",
       " (20662, 0.7684177756309509),\n",
       " (29602, 0.7683101296424866),\n",
       " (3357, 0.7682665586471558),\n",
       " (17633, 0.7681795358657837),\n",
       " (6473, 0.768147885799408),\n",
       " (9300, 0.7680162787437439),\n",
       " (8909, 0.7679756879806519),\n",
       " (14917, 0.7678909301757812),\n",
       " (3981, 0.7678858041763306),\n",
       " (33843, 0.7677796483039856),\n",
       " (31181, 0.767722487449646),\n",
       " (27099, 0.7675832509994507),\n",
       " (20534, 0.7675414681434631),\n",
       " (13543, 0.7674597501754761),\n",
       " (12054, 0.7674165964126587),\n",
       " (33139, 0.767285168170929),\n",
       " (24986, 0.7671889066696167),\n",
       " (14309, 0.766832709312439),\n",
       " (33676, 0.7665879130363464),\n",
       " (5014, 0.7665792107582092),\n",
       " (13191, 0.7665674090385437),\n",
       " (11055, 0.7665239572525024),\n",
       " (2967, 0.766480028629303),\n",
       " (11700, 0.7664762735366821),\n",
       " (4928, 0.7663255929946899),\n",
       " (11090, 0.7663161158561707),\n",
       " (14324, 0.7662879228591919),\n",
       " (20571, 0.7662558555603027),\n",
       " (12080, 0.7662348747253418),\n",
       " (6126, 0.7661337852478027),\n",
       " (3831, 0.7659200429916382),\n",
       " (17842, 0.7659115195274353),\n",
       " (13472, 0.7657922506332397),\n",
       " (13400, 0.7657254934310913),\n",
       " (6460, 0.7656241655349731),\n",
       " (11965, 0.7653255462646484),\n",
       " (11603, 0.7652355432510376),\n",
       " (3144, 0.7652206420898438),\n",
       " (23651, 0.7651890516281128),\n",
       " (22671, 0.7651528120040894),\n",
       " (23422, 0.7650172710418701),\n",
       " (20289, 0.7649932503700256),\n",
       " (29547, 0.7647634744644165),\n",
       " (24076, 0.7647478580474854),\n",
       " (11519, 0.7647457122802734),\n",
       " (7927, 0.7646884918212891),\n",
       " (3021, 0.7646596431732178),\n",
       " (14616, 0.7646528482437134),\n",
       " (33523, 0.7645896673202515),\n",
       " (13155, 0.7645721435546875),\n",
       " (11290, 0.7644981145858765),\n",
       " (1985, 0.7644881010055542),\n",
       " (30855, 0.7644494771957397),\n",
       " (2351, 0.7643979787826538),\n",
       " (14101, 0.764113187789917),\n",
       " (15781, 0.7639697790145874),\n",
       " (458, 0.7638267278671265),\n",
       " (6931, 0.7638241052627563),\n",
       " (6607, 0.7635431289672852),\n",
       " (955, 0.7634965181350708),\n",
       " (16984, 0.7633770704269409),\n",
       " (19438, 0.7633306980133057),\n",
       " (20136, 0.7632186412811279),\n",
       " (25409, 0.7631982564926147),\n",
       " (4086, 0.7630975246429443),\n",
       " (24053, 0.7630569338798523),\n",
       " (7290, 0.7630165219306946),\n",
       " (11266, 0.7627768516540527),\n",
       " (19331, 0.7626838088035583),\n",
       " (14681, 0.7625539302825928),\n",
       " (7164, 0.7625070810317993),\n",
       " (24061, 0.7624779343605042),\n",
       " (24834, 0.7624753713607788),\n",
       " (28142, 0.7624183893203735),\n",
       " (10585, 0.7623289823532104),\n",
       " (15559, 0.7622123956680298),\n",
       " (6153, 0.762174665927887),\n",
       " (11310, 0.7621321678161621),\n",
       " (503, 0.7621316909790039),\n",
       " (11913, 0.7621264457702637),\n",
       " (28421, 0.7620859742164612),\n",
       " (27545, 0.7619800567626953),\n",
       " (28547, 0.7619258165359497),\n",
       " (18491, 0.7618831992149353),\n",
       " (4507, 0.7618441581726074),\n",
       " (12526, 0.761662483215332),\n",
       " (22547, 0.7615647315979004),\n",
       " (14924, 0.7615635991096497),\n",
       " (21710, 0.7615060806274414),\n",
       " (15987, 0.7613837718963623),\n",
       " (15931, 0.7613411545753479),\n",
       " (32206, 0.7613357305526733),\n",
       " (11039, 0.7612972259521484),\n",
       " (16713, 0.7612279653549194),\n",
       " (4165, 0.761192262172699),\n",
       " (17837, 0.7611113786697388),\n",
       " (2127, 0.7611016035079956),\n",
       " (32172, 0.7610772848129272),\n",
       " (13516, 0.7610312700271606),\n",
       " (17163, 0.7609075307846069),\n",
       " (21211, 0.7608661651611328),\n",
       " (5955, 0.7608294486999512),\n",
       " (10938, 0.7607868909835815),\n",
       " (5688, 0.7607731223106384),\n",
       " (5100, 0.7606671452522278),\n",
       " (1905, 0.760488748550415),\n",
       " (7417, 0.7604880332946777),\n",
       " (29458, 0.7603791952133179),\n",
       " (20422, 0.7603319883346558),\n",
       " (10489, 0.7602735757827759),\n",
       " (31061, 0.7601733803749084),\n",
       " (30059, 0.7601170539855957),\n",
       " (3318, 0.7599515914916992),\n",
       " (4649, 0.7599272727966309),\n",
       " (11777, 0.7598767280578613),\n",
       " (9204, 0.759764552116394),\n",
       " (3758, 0.7597503066062927),\n",
       " (27272, 0.75947105884552),\n",
       " (22662, 0.759388267993927),\n",
       " (9894, 0.7593370676040649),\n",
       " (17375, 0.7593281269073486),\n",
       " (11000, 0.7593218088150024),\n",
       " (28094, 0.7590669393539429),\n",
       " (26225, 0.7590659856796265),\n",
       " (11701, 0.7590535879135132),\n",
       " (11054, 0.75900799036026),\n",
       " (1112, 0.7589412331581116),\n",
       " (13102, 0.7589313983917236),\n",
       " (4236, 0.7589132785797119),\n",
       " (4193, 0.7588979005813599),\n",
       " (28128, 0.7586912512779236),\n",
       " (25432, 0.7585810422897339),\n",
       " (1982, 0.7584601044654846),\n",
       " (19622, 0.7579600811004639),\n",
       " (11935, 0.7579256892204285),\n",
       " (21548, 0.757830798625946),\n",
       " (12267, 0.7578185200691223),\n",
       " (8281, 0.7577626705169678),\n",
       " (28382, 0.7577214241027832),\n",
       " (22844, 0.7575196027755737),\n",
       " (24567, 0.7574764490127563),\n",
       " (3138, 0.7573964595794678),\n",
       " (2117, 0.7573262453079224),\n",
       " (29113, 0.7573111057281494),\n",
       " (10251, 0.7571573257446289),\n",
       " (31827, 0.7570668458938599),\n",
       " (28456, 0.757040798664093),\n",
       " (10863, 0.7568744421005249),\n",
       " (24006, 0.7568645477294922),\n",
       " (19212, 0.7568507194519043),\n",
       " (10796, 0.7567489743232727),\n",
       " (25335, 0.7566739320755005),\n",
       " (7630, 0.7565785646438599),\n",
       " (11050, 0.7563340663909912),\n",
       " (25216, 0.7561848163604736),\n",
       " (28161, 0.7561399340629578),\n",
       " (10639, 0.7560853362083435),\n",
       " (34785, 0.7560765147209167),\n",
       " (31, 0.755974292755127),\n",
       " (1226, 0.7559611797332764),\n",
       " (28691, 0.755955159664154),\n",
       " (9149, 0.7558356523513794),\n",
       " (6884, 0.7557446360588074),\n",
       " (21170, 0.755715012550354),\n",
       " (7361, 0.7556954622268677),\n",
       " (9534, 0.755605936050415),\n",
       " (34561, 0.7555528283119202),\n",
       " (6645, 0.7554649114608765),\n",
       " (18421, 0.755370020866394),\n",
       " (530, 0.755183756351471),\n",
       " (16979, 0.7551238536834717),\n",
       " (16665, 0.7550790905952454),\n",
       " (23201, 0.7550157904624939),\n",
       " (19405, 0.7550021409988403),\n",
       " (28855, 0.754851222038269),\n",
       " (14693, 0.7548294067382812),\n",
       " (9684, 0.7547276616096497),\n",
       " (18097, 0.7546727657318115),\n",
       " (12129, 0.7546172142028809),\n",
       " (33560, 0.7545576095581055),\n",
       " (24838, 0.7544543743133545),\n",
       " (20993, 0.7544198036193848),\n",
       " (7292, 0.7543387413024902),\n",
       " (18708, 0.7543350458145142),\n",
       " (11512, 0.7541788816452026),\n",
       " (6436, 0.7540414929389954),\n",
       " (9270, 0.754040002822876),\n",
       " (14403, 0.7540206909179688),\n",
       " (28068, 0.75400710105896),\n",
       " (23837, 0.7539851665496826),\n",
       " (14677, 0.7539624571800232),\n",
       " (29713, 0.7539253830909729),\n",
       " (4061, 0.7539174556732178),\n",
       " (26364, 0.7538378238677979),\n",
       " (3131, 0.7537685632705688),\n",
       " (3892, 0.7537593245506287),\n",
       " (9599, 0.7536842823028564),\n",
       " (17780, 0.7535378932952881),\n",
       " (11980, 0.7530803680419922),\n",
       " (11477, 0.7530481219291687),\n",
       " (22155, 0.7527585625648499),\n",
       " (26935, 0.7527103424072266),\n",
       " (8270, 0.7526392936706543),\n",
       " (33102, 0.7524493932723999),\n",
       " (5655, 0.7522721886634827),\n",
       " (30840, 0.7521427273750305),\n",
       " (20395, 0.752121090888977),\n",
       " (34953, 0.7521209120750427),\n",
       " (11572, 0.7521005868911743),\n",
       " (3080, 0.7520764470100403),\n",
       " (16402, 0.7519801259040833),\n",
       " (2397, 0.75187087059021),\n",
       " (15699, 0.7518091201782227),\n",
       " (27997, 0.7517845034599304),\n",
       " (24733, 0.7516716718673706),\n",
       " (12576, 0.7516611814498901),\n",
       " (4525, 0.7515323758125305),\n",
       " (10229, 0.7514904737472534),\n",
       " (19406, 0.7514769434928894),\n",
       " (3215, 0.7513517141342163),\n",
       " (29778, 0.7512949705123901),\n",
       " (4127, 0.7512669563293457),\n",
       " (14942, 0.7512079477310181),\n",
       " (5847, 0.7511434555053711),\n",
       " (15179, 0.7511224746704102),\n",
       " (11490, 0.7510228157043457),\n",
       " (16332, 0.7510138154029846),\n",
       " (9611, 0.7509992122650146),\n",
       " (21653, 0.750542402267456),\n",
       " (24709, 0.7504879832267761),\n",
       " (9419, 0.7504540681838989),\n",
       " (1908, 0.7504485249519348),\n",
       " (27561, 0.7503869533538818),\n",
       " (29393, 0.7503381967544556),\n",
       " (20336, 0.7502808570861816),\n",
       " (11182, 0.7502529621124268),\n",
       " (24431, 0.7502030730247498),\n",
       " (16436, 0.7501587867736816),\n",
       " (28859, 0.7499247789382935),\n",
       " (17729, 0.7498481273651123),\n",
       " (14204, 0.7498455047607422),\n",
       " (10940, 0.749737560749054),\n",
       " (7548, 0.7496185302734375),\n",
       " (3617, 0.7495368719100952),\n",
       " (16331, 0.7494995594024658),\n",
       " (34808, 0.7494789361953735),\n",
       " (19699, 0.7493524551391602),\n",
       " (15566, 0.749337911605835),\n",
       " (4987, 0.7490706443786621),\n",
       " (9179, 0.7489573955535889),\n",
       " (5799, 0.7489560842514038),\n",
       " (5347, 0.7489490509033203),\n",
       " (1247, 0.7489213943481445),\n",
       " (15678, 0.7489033937454224),\n",
       " (34044, 0.7487978935241699),\n",
       " (16321, 0.7487786412239075),\n",
       " (25795, 0.748761773109436),\n",
       " (32597, 0.7487547397613525),\n",
       " (28455, 0.7487120628356934),\n",
       " (18087, 0.7486374378204346),\n",
       " (25756, 0.7486039400100708),\n",
       " (19204, 0.7486030459403992),\n",
       " (9590, 0.7485705614089966),\n",
       " (7657, 0.7485222220420837),\n",
       " (8342, 0.7485024333000183),\n",
       " (33345, 0.7484204769134521),\n",
       " (10329, 0.7482959628105164),\n",
       " (21535, 0.7482393980026245),\n",
       " (19768, 0.7482219934463501),\n",
       " (29063, 0.7481920123100281),\n",
       " (32702, 0.7478832006454468),\n",
       " (16788, 0.7478525638580322),\n",
       " (22666, 0.7477773427963257),\n",
       " (28201, 0.7477574944496155),\n",
       " (15703, 0.7477458715438843),\n",
       " (13866, 0.7477414608001709),\n",
       " (1430, 0.7476871013641357),\n",
       " (33122, 0.7474833130836487),\n",
       " (25367, 0.7474691867828369),\n",
       " (19872, 0.7470551133155823),\n",
       " (17865, 0.7470244765281677),\n",
       " (28535, 0.7468851804733276),\n",
       " (9789, 0.7468210458755493),\n",
       " (20383, 0.7467439770698547),\n",
       " (22245, 0.7466780543327332),\n",
       " (9850, 0.7465687990188599),\n",
       " (30734, 0.7464933395385742),\n",
       " (6732, 0.7464650869369507),\n",
       " (16486, 0.7464479207992554),\n",
       " (15373, 0.7463598251342773),\n",
       " (31019, 0.7463372945785522),\n",
       " (33915, 0.7463338375091553),\n",
       " (28858, 0.7463165521621704),\n",
       " (12537, 0.7462467551231384),\n",
       " (19609, 0.7461492419242859),\n",
       " (13409, 0.7461094856262207),\n",
       " (3686, 0.7460769414901733),\n",
       " (12828, 0.7460706233978271),\n",
       " (814, 0.7459908127784729),\n",
       " (9638, 0.7459853887557983),\n",
       " (10705, 0.7459803819656372),\n",
       " (20669, 0.7458888292312622),\n",
       " (13445, 0.7458454370498657),\n",
       " (3141, 0.745638370513916),\n",
       " (30125, 0.7456167936325073),\n",
       " (15733, 0.7455748319625854),\n",
       " (6728, 0.7454808354377747),\n",
       " (17857, 0.7453947067260742),\n",
       " (9628, 0.7453898191452026),\n",
       " (6986, 0.7453440427780151),\n",
       " (28164, 0.7452678084373474),\n",
       " (34456, 0.7452061772346497),\n",
       " (28556, 0.7451153993606567),\n",
       " (1945, 0.7448210120201111),\n",
       " (15349, 0.7446912527084351),\n",
       " (11695, 0.7446119785308838),\n",
       " (9425, 0.7445731163024902),\n",
       " (24988, 0.7445502281188965),\n",
       " (33851, 0.7445484399795532),\n",
       " (10985, 0.744486927986145),\n",
       " (19904, 0.7443954944610596),\n",
       " (16433, 0.7443419694900513),\n",
       " (11815, 0.7442558407783508),\n",
       " (27463, 0.7442418932914734),\n",
       " (25124, 0.7442084550857544),\n",
       " (27598, 0.7441960573196411),\n",
       " (6194, 0.7441779375076294),\n",
       " (3219, 0.7441179752349854),\n",
       " (13038, 0.7441161274909973),\n",
       " (18185, 0.7438762784004211),\n",
       " (15347, 0.743843138217926),\n",
       " (31936, 0.7438347339630127),\n",
       " (21033, 0.7438287734985352),\n",
       " (22865, 0.7438067197799683),\n",
       " (11437, 0.7437235116958618),\n",
       " (24928, 0.7436951398849487),\n",
       " (6626, 0.743659496307373),\n",
       " (9325, 0.7436283826828003),\n",
       " (29387, 0.7435811161994934),\n",
       " (1597, 0.7435339689254761),\n",
       " (1284, 0.7435266375541687),\n",
       " (14322, 0.7435094714164734),\n",
       " (16087, 0.7434827089309692),\n",
       " (2766, 0.7434547543525696),\n",
       " (2510, 0.7433552742004395),\n",
       " (27018, 0.7432913184165955),\n",
       " (4135, 0.7432719469070435),\n",
       " (11052, 0.7432433366775513),\n",
       " (16252, 0.7432165145874023),\n",
       " (3412, 0.7431195974349976),\n",
       " (18646, 0.7429158687591553),\n",
       " (10421, 0.742870569229126),\n",
       " (9336, 0.7428048849105835),\n",
       " (16755, 0.7427726984024048),\n",
       " (32985, 0.7426940202713013),\n",
       " (13081, 0.7426621913909912),\n",
       " (1450, 0.7426501512527466),\n",
       " (30192, 0.7426345348358154),\n",
       " (8597, 0.742624044418335),\n",
       " (16191, 0.7426189184188843),\n",
       " (1859, 0.7425621747970581),\n",
       " (34445, 0.7425521016120911),\n",
       " (15997, 0.742456316947937),\n",
       " (27244, 0.7423104047775269),\n",
       " (14794, 0.7422816753387451),\n",
       " (6459, 0.7422709465026855),\n",
       " (12532, 0.7422699928283691),\n",
       " (31459, 0.7422002553939819),\n",
       " (21790, 0.7421766519546509),\n",
       " (10328, 0.7420991659164429),\n",
       " (14492, 0.7420781850814819),\n",
       " (21901, 0.742075502872467),\n",
       " (9335, 0.7419232130050659),\n",
       " (3628, 0.7419143915176392),\n",
       " (4116, 0.741783082485199),\n",
       " (16419, 0.7416812181472778),\n",
       " (21463, 0.7415540814399719),\n",
       " (18853, 0.7415399551391602),\n",
       " (12967, 0.7414906024932861),\n",
       " (2147, 0.7414628267288208),\n",
       " (2324, 0.7413538694381714),\n",
       " (3576, 0.7413464188575745),\n",
       " (16429, 0.7412956953048706),\n",
       " (22617, 0.7411161065101624),\n",
       " (31866, 0.7410323619842529),\n",
       " (28279, 0.7410244941711426),\n",
       " (1944, 0.7408958077430725),\n",
       " (1296, 0.7408953905105591),\n",
       " (27037, 0.7406980991363525),\n",
       " (20578, 0.7406879663467407),\n",
       " (10538, 0.7406286001205444),\n",
       " (5805, 0.7406018972396851),\n",
       " (25217, 0.7405571937561035),\n",
       " (3359, 0.740500807762146),\n",
       " (3260, 0.7404961585998535),\n",
       " (1590, 0.7404330968856812),\n",
       " (18135, 0.7402973175048828),\n",
       " (7721, 0.7402763366699219),\n",
       " (12641, 0.740126371383667),\n",
       " (26847, 0.7401175498962402),\n",
       " (864, 0.7400648593902588),\n",
       " (26614, 0.7399814128875732),\n",
       " (32989, 0.7399806380271912),\n",
       " (23950, 0.7399581074714661),\n",
       " (28915, 0.7398450374603271),\n",
       " (34055, 0.7398282885551453),\n",
       " (5585, 0.7398083209991455),\n",
       " (5619, 0.7396622896194458),\n",
       " (33782, 0.7396622896194458),\n",
       " (29398, 0.7395859956741333),\n",
       " (9545, 0.7395768165588379),\n",
       " (4464, 0.7395648956298828),\n",
       " (2888, 0.7395336627960205),\n",
       " (11981, 0.7394773960113525),\n",
       " (10706, 0.7394248247146606),\n",
       " (22657, 0.7393969297409058),\n",
       " (13814, 0.7393426895141602),\n",
       " (24640, 0.7392117977142334),\n",
       " (18560, 0.7390642166137695),\n",
       " (12902, 0.7390586137771606),\n",
       " (7014, 0.739039421081543),\n",
       " (10207, 0.7388930320739746),\n",
       " (4100, 0.7387229204177856),\n",
       " (18110, 0.7386645674705505),\n",
       " (26109, 0.7386560440063477),\n",
       " (7168, 0.738555908203125),\n",
       " (7553, 0.7385361194610596),\n",
       " (18760, 0.7385326027870178),\n",
       " (15313, 0.7384846210479736),\n",
       " (31527, 0.7384599447250366),\n",
       " (14650, 0.7384558916091919),\n",
       " (28857, 0.7383227348327637),\n",
       " (1651, 0.738295316696167),\n",
       " (16980, 0.7382842898368835),\n",
       " (28427, 0.7380889654159546),\n",
       " (18227, 0.7379799485206604),\n",
       " (2418, 0.737967312335968),\n",
       " (19342, 0.7379451394081116),\n",
       " (1236, 0.7379175424575806),\n",
       " (26549, 0.7379120588302612),\n",
       " (8616, 0.7378805875778198),\n",
       " (28903, 0.7378610372543335),\n",
       " (9594, 0.7377883195877075),\n",
       " (31401, 0.7377523183822632),\n",
       " (12068, 0.7376656532287598),\n",
       " (8643, 0.7376335859298706),\n",
       " (10109, 0.7376033067703247),\n",
       " (9865, 0.7374739646911621),\n",
       " (13495, 0.7372899055480957),\n",
       " (10090, 0.7371366620063782),\n",
       " (7544, 0.7370782494544983),\n",
       " (30286, 0.7369461059570312),\n",
       " (9920, 0.7368738055229187),\n",
       " (8778, 0.7368274331092834),\n",
       " (25872, 0.7367159128189087),\n",
       " (8054, 0.7366335391998291),\n",
       " (33846, 0.7365903854370117),\n",
       " (844, 0.7365717887878418),\n",
       " (21836, 0.7363665103912354),\n",
       " (31050, 0.7363590002059937),\n",
       " (15143, 0.7363438606262207),\n",
       " (29560, 0.7362957000732422),\n",
       " (25358, 0.7362322807312012),\n",
       " (2638, 0.7362291812896729),\n",
       " (18808, 0.7360923290252686),\n",
       " (27344, 0.7358094453811646),\n",
       " (23025, 0.7357587814331055),\n",
       " (3853, 0.7355632185935974),\n",
       " (34582, 0.7355272173881531),\n",
       " (25478, 0.7355120182037354),\n",
       " (18882, 0.7354576587677002),\n",
       " (8331, 0.7354251146316528),\n",
       " (11137, 0.7354117631912231),\n",
       " (12878, 0.7353405952453613),\n",
       " (13586, 0.735292375087738),\n",
       " (22584, 0.7352392673492432),\n",
       " (32136, 0.73520427942276),\n",
       " (27946, 0.7351346015930176),\n",
       " (1027, 0.7351133227348328),\n",
       " (11468, 0.7350785732269287),\n",
       " (2798, 0.7350008487701416),\n",
       " (16255, 0.7349629402160645),\n",
       " (33126, 0.7346894145011902),\n",
       " (26050, 0.7346657514572144),\n",
       " (19692, 0.7346574664115906),\n",
       " (34676, 0.734650731086731),\n",
       " (4978, 0.7345569133758545),\n",
       " (34674, 0.7345426082611084),\n",
       " (33575, 0.7344704270362854),\n",
       " (34017, 0.7344547510147095),\n",
       " (7341, 0.7343879342079163),\n",
       " (10357, 0.7342948913574219),\n",
       " (4504, 0.7341060638427734),\n",
       " (8862, 0.7340682148933411),\n",
       " (24697, 0.7340568900108337),\n",
       " (839, 0.734041154384613),\n",
       " (9166, 0.7339431047439575),\n",
       " (27477, 0.7339402437210083),\n",
       " (7978, 0.7337313294410706),\n",
       " (22910, 0.7337255477905273),\n",
       " (17503, 0.7337180376052856),\n",
       " (14371, 0.7336851358413696),\n",
       " (18975, 0.7336592674255371),\n",
       " (16053, 0.7335214614868164),\n",
       " (1742, 0.7334957122802734),\n",
       " (25666, 0.733410120010376),\n",
       " (1662, 0.7333911657333374),\n",
       " (23343, 0.7332510948181152),\n",
       " (18192, 0.7332442998886108),\n",
       " (1780, 0.7332097291946411),\n",
       " (33422, 0.733207106590271),\n",
       " (25403, 0.7332034111022949),\n",
       " (19509, 0.7332025766372681),\n",
       " (23833, 0.7331783771514893),\n",
       " (5915, 0.7330133318901062),\n",
       " (101, 0.7330105304718018),\n",
       " (9429, 0.7329868078231812),\n",
       " (16726, 0.7329829931259155),\n",
       " (14630, 0.7329440712928772),\n",
       " (5851, 0.732900857925415),\n",
       " (31575, 0.7328907251358032),\n",
       " (11745, 0.7328256368637085),\n",
       " (13796, 0.7327840328216553),\n",
       " (13544, 0.7327300310134888),\n",
       " (10922, 0.7326973080635071),\n",
       " (25786, 0.7326492071151733),\n",
       " (9897, 0.7325955629348755),\n",
       " (8295, 0.7325650453567505),\n",
       " (3266, 0.7325369715690613),\n",
       " (8370, 0.7325165867805481),\n",
       " (20547, 0.7325047254562378),\n",
       " (28546, 0.7324793338775635),\n",
       " (18750, 0.7324428558349609),\n",
       " (8345, 0.7323439121246338),\n",
       " (20792, 0.7322641611099243),\n",
       " (132, 0.732215166091919),\n",
       " (2745, 0.7321466207504272),\n",
       " (28551, 0.7320919632911682),\n",
       " (21182, 0.7320668697357178),\n",
       " (22847, 0.7319854497909546),\n",
       " (34956, 0.7319672107696533),\n",
       " (8656, 0.7319247722625732),\n",
       " (11735, 0.7317173480987549),\n",
       " (2610, 0.7314895391464233),\n",
       " (28396, 0.7314420938491821),\n",
       " (18130, 0.7313358783721924),\n",
       " (22845, 0.7313083410263062),\n",
       " (10613, 0.7312277555465698),\n",
       " (24206, 0.7309474945068359),\n",
       " (10803, 0.7308781743049622),\n",
       " (11076, 0.7308744192123413),\n",
       " (25147, 0.7307933568954468),\n",
       " (33282, 0.7307878732681274),\n",
       " (8744, 0.7307809591293335),\n",
       " (9032, 0.7307313084602356),\n",
       " (15308, 0.7306195497512817),\n",
       " (30598, 0.7306053638458252),\n",
       " (29361, 0.7305631041526794),\n",
       " (9694, 0.7305392622947693),\n",
       " (24290, 0.730461597442627),\n",
       " (22491, 0.7304153442382812),\n",
       " (18681, 0.7303628921508789),\n",
       " (27560, 0.7303594350814819),\n",
       " (9635, 0.7302743196487427),\n",
       " (26554, 0.7302317023277283),\n",
       " (29705, 0.7300267815589905),\n",
       " (30225, 0.7300143241882324),\n",
       " (1445, 0.7300113439559937),\n",
       " (19387, 0.7299858331680298),\n",
       " (13499, 0.7299002408981323),\n",
       " (24000, 0.7296908497810364),\n",
       " (16297, 0.7295442819595337),\n",
       " (26418, 0.7295049428939819),\n",
       " (24178, 0.7292847633361816),\n",
       " (16355, 0.7292746305465698),\n",
       " (19229, 0.7292706370353699),\n",
       " (18958, 0.729202151298523),\n",
       " (32980, 0.7291719317436218),\n",
       " (25914, 0.7291425466537476),\n",
       " (32917, 0.7289407253265381),\n",
       " (6112, 0.7288304567337036),\n",
       " (17665, 0.7287646532058716),\n",
       " (29003, 0.7287088632583618),\n",
       " (10871, 0.7286956310272217),\n",
       " (31218, 0.7285220623016357),\n",
       " (20116, 0.7285112142562866),\n",
       " (3204, 0.7284177541732788),\n",
       " (20122, 0.728367805480957),\n",
       " (31828, 0.7283494472503662),\n",
       " (276, 0.7283166646957397),\n",
       " (16382, 0.7282742261886597),\n",
       " (16258, 0.7282581329345703),\n",
       " (25885, 0.7281649112701416),\n",
       " (16497, 0.7281131744384766),\n",
       " (20228, 0.7280744910240173),\n",
       " (20732, 0.7280350923538208),\n",
       " (2649, 0.7280000448226929),\n",
       " (15933, 0.7279216051101685),\n",
       " (28345, 0.7278846502304077),\n",
       " (27530, 0.727864146232605),\n",
       " (4587, 0.7278582453727722),\n",
       " (9187, 0.7276455760002136),\n",
       " (34177, 0.7275369167327881),\n",
       " (26846, 0.72730553150177),\n",
       " (3241, 0.7272379398345947),\n",
       " (10671, 0.7271643877029419),\n",
       " (25752, 0.7270267009735107),\n",
       " (17553, 0.7270119190216064),\n",
       " (30281, 0.7269930839538574),\n",
       " (23836, 0.7269670963287354),\n",
       " (10742, 0.7269250154495239),\n",
       " (28680, 0.7268782258033752),\n",
       " (8211, 0.7267749309539795),\n",
       " (2218, 0.7267297506332397),\n",
       " (9178, 0.7267075777053833),\n",
       " (29825, 0.7266160845756531),\n",
       " (1312, 0.7266016602516174),\n",
       " (18114, 0.7265611886978149),\n",
       " (10961, 0.7265146970748901),\n",
       " (16207, 0.7264878153800964),\n",
       " (31052, 0.7264856696128845),\n",
       " (4699, 0.7263914346694946),\n",
       " (12934, 0.7262980937957764),\n",
       " (34368, 0.726294994354248),\n",
       " (17532, 0.7262702584266663),\n",
       " (3362, 0.7262101769447327),\n",
       " (1589, 0.726147472858429),\n",
       " (27042, 0.7261109352111816),\n",
       " (13455, 0.7260158061981201),\n",
       " (26509, 0.7259052991867065),\n",
       " (17491, 0.7258940935134888),\n",
       " (11994, 0.725867509841919),\n",
       " (4665, 0.7256674766540527),\n",
       " (22630, 0.7256529927253723),\n",
       " (30730, 0.7255827188491821),\n",
       " (9617, 0.7255440950393677),\n",
       " (28139, 0.7254376411437988),\n",
       " (21921, 0.7253521680831909),\n",
       " (34376, 0.7251598238945007),\n",
       " (7336, 0.7251595258712769),\n",
       " (19057, 0.724956750869751),\n",
       " (16965, 0.7249132394790649),\n",
       " (11165, 0.7248595952987671),\n",
       " (18793, 0.7247827053070068),\n",
       " (975, 0.7247737646102905),\n",
       " (9892, 0.7247613072395325),\n",
       " (19144, 0.7247133255004883),\n",
       " (7026, 0.7247132062911987),\n",
       " (14874, 0.7247067093849182),\n",
       " (12025, 0.7245743274688721),\n",
       " (30564, 0.7245142459869385),\n",
       " (1025, 0.724511444568634),\n",
       " (4676, 0.7244957089424133),\n",
       " (13415, 0.7244830131530762),\n",
       " (2162, 0.7242425680160522),\n",
       " (3548, 0.7241621613502502),\n",
       " (10056, 0.7241435050964355),\n",
       " (28943, 0.724128782749176),\n",
       " (4624, 0.72411048412323),\n",
       " (25852, 0.7240383625030518),\n",
       " (20618, 0.7239793539047241),\n",
       " (5855, 0.723965048789978),\n",
       " (11726, 0.7239552736282349),\n",
       " (20394, 0.723892331123352),\n",
       " (131, 0.7236303091049194),\n",
       " (15174, 0.723505437374115),\n",
       " (28664, 0.7234925031661987),\n",
       " (27566, 0.723455548286438),\n",
       " (14286, 0.723294198513031),\n",
       " (9671, 0.7232657670974731),\n",
       " (19537, 0.7232558727264404),\n",
       " (15976, 0.7232137322425842),\n",
       " (28202, 0.7232104539871216),\n",
       " (5397, 0.7231868505477905),\n",
       " (6245, 0.7231671810150146),\n",
       " (19818, 0.7231196761131287),\n",
       " (2443, 0.7231050729751587),\n",
       " (9833, 0.7230262756347656),\n",
       " (12214, 0.7229604721069336),\n",
       " (34167, 0.7228171825408936),\n",
       " (1719, 0.7227955460548401),\n",
       " (21701, 0.7227905988693237),\n",
       " (5484, 0.7226994037628174),\n",
       " (30561, 0.7226337194442749),\n",
       " (2689, 0.7225984930992126),\n",
       " (4215, 0.7225828170776367),\n",
       " (16488, 0.7225622534751892),\n",
       " (26200, 0.7225080728530884),\n",
       " (29980, 0.7224118709564209),\n",
       " (11966, 0.7224080562591553),\n",
       " (5492, 0.7222706079483032),\n",
       " (18151, 0.7222561836242676),\n",
       " (18298, 0.7221436500549316),\n",
       " (10175, 0.7220642566680908),\n",
       " (19534, 0.7220277786254883),\n",
       " (2678, 0.7220100164413452),\n",
       " (9629, 0.7219840288162231),\n",
       " (34842, 0.7219840288162231),\n",
       " (25414, 0.7219525575637817),\n",
       " (13448, 0.7219366431236267),\n",
       " (34604, 0.7218567132949829),\n",
       " (21396, 0.7218456268310547),\n",
       " (30711, 0.7217891812324524),\n",
       " (16934, 0.7217821478843689),\n",
       " (33044, 0.721706748008728),\n",
       " (9633, 0.7217051386833191),\n",
       " (13670, 0.7216299772262573),\n",
       " (4714, 0.7216280102729797),\n",
       " (5901, 0.7215651273727417),\n",
       " (17826, 0.7215233445167542),\n",
       " (15468, 0.7214694023132324),\n",
       " (18021, 0.7214149236679077),\n",
       " (24398, 0.7214117050170898),\n",
       " (1204, 0.7213273048400879),\n",
       " (7840, 0.7212613224983215),\n",
       " (7715, 0.7212266325950623),\n",
       " (17909, 0.7212038040161133),\n",
       " (18811, 0.7211284637451172),\n",
       " (5824, 0.7210888266563416),\n",
       " (34675, 0.7210431098937988),\n",
       " (8340, 0.7210299968719482),\n",
       " (25482, 0.7209504842758179),\n",
       " (23702, 0.7208489775657654),\n",
       " (15737, 0.7207028269767761),\n",
       " (17646, 0.7206457257270813),\n",
       " (32937, 0.720536470413208),\n",
       " (14652, 0.7205346822738647),\n",
       " (6426, 0.7205296158790588),\n",
       " (1426, 0.7205044627189636),\n",
       " (24037, 0.7204875946044922),\n",
       " (7131, 0.7204822301864624),\n",
       " (3164, 0.720362663269043),\n",
       " (33444, 0.7202886343002319),\n",
       " (3787, 0.7201895713806152),\n",
       " (16623, 0.7200919985771179),\n",
       " (28498, 0.7199949026107788),\n",
       " (20407, 0.7199796438217163),\n",
       " (11020, 0.7199023365974426),\n",
       " (1892, 0.7198022603988647),\n",
       " (19440, 0.7197245359420776),\n",
       " (30483, 0.7196166515350342),\n",
       " (23992, 0.7195291519165039),\n",
       " (19129, 0.719516396522522),\n",
       " (24101, 0.7194935083389282),\n",
       " (32819, 0.7194876670837402),\n",
       " (25091, 0.7194792032241821),\n",
       " (22967, 0.7194721698760986),\n",
       " (4894, 0.7194695472717285),\n",
       " (20253, 0.71946120262146),\n",
       " (10010, 0.7194501161575317),\n",
       " (10935, 0.7194119691848755),\n",
       " (32191, 0.7193120718002319),\n",
       " (33503, 0.7192736864089966),\n",
       " (12805, 0.7192479372024536),\n",
       " (32466, 0.7192190885543823),\n",
       " (1086, 0.7191454172134399),\n",
       " (10173, 0.7191264033317566),\n",
       " (22405, 0.7188949584960938),\n",
       " (15325, 0.7188107371330261),\n",
       " (33533, 0.7187472581863403),\n",
       " (32711, 0.7186935544013977),\n",
       " (25039, 0.7186280488967896),\n",
       " (32924, 0.7185966372489929),\n",
       " (27691, 0.7185644507408142),\n",
       " (14370, 0.7185442447662354),\n",
       " (2441, 0.7185142636299133),\n",
       " (6999, 0.7184878587722778),\n",
       " (9893, 0.7184516191482544),\n",
       " (24374, 0.718413233757019),\n",
       " (20134, 0.7184089422225952),\n",
       " (5006, 0.7183198928833008),\n",
       " (1805, 0.718183159828186),\n",
       " (26544, 0.7180930972099304),\n",
       " (10444, 0.7180584669113159),\n",
       " (16142, 0.7180309295654297),\n",
       " (9527, 0.7179862260818481),\n",
       " (7778, 0.7179756164550781),\n",
       " (18512, 0.7179622650146484),\n",
       " (332, 0.7179197072982788),\n",
       " (29859, 0.7178835272789001),\n",
       " (13930, 0.7178634405136108),\n",
       " (20633, 0.7178144454956055),\n",
       " (31576, 0.7178137898445129),\n",
       " (9674, 0.7177925109863281),\n",
       " (22428, 0.717653751373291),\n",
       " (11421, 0.7174837589263916),\n",
       " (4334, 0.7174210548400879),\n",
       " (32376, 0.71722412109375),\n",
       " (24261, 0.7170642614364624),\n",
       " (27944, 0.7170493006706238),\n",
       " (25190, 0.7169656753540039),\n",
       " (33573, 0.7169461250305176),\n",
       " (9508, 0.7169314622879028),\n",
       " (13539, 0.716888427734375),\n",
       " (20142, 0.7168879508972168),\n",
       " (18879, 0.7167712450027466),\n",
       " (4394, 0.7167646884918213),\n",
       " (2130, 0.7166429162025452),\n",
       " (24259, 0.7166078090667725),\n",
       " (16670, 0.7165292501449585),\n",
       " (26144, 0.7163375616073608),\n",
       " (30962, 0.7162026166915894),\n",
       " (25713, 0.7161608338356018),\n",
       " (12680, 0.7161391973495483),\n",
       " (27264, 0.7161358594894409),\n",
       " (21818, 0.7160801887512207),\n",
       " (1799, 0.7160734534263611),\n",
       " (23710, 0.716042160987854),\n",
       " ...]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.docvecs.most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“There could be no harm in her liking an agreeable man—everybody knew her situation—Mr. Crawford must take care of himself”.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austen_sentences[21362]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2299,\n",
       " 1939,\n",
       " 17904,\n",
       " 109,\n",
       " 11505,\n",
       " 29772,\n",
       " 7512,\n",
       " 2578,\n",
       " 529,\n",
       " 21362,\n",
       " 22195,\n",
       " 1082,\n",
       " 40,\n",
       " 4156,\n",
       " 15087,\n",
       " 448,\n",
       " 5591,\n",
       " 761,\n",
       " 663,\n",
       " 21367,\n",
       " 12760,\n",
       " 129,\n",
       " 7443,\n",
       " 16190,\n",
       " 8,\n",
       " 20250,\n",
       " 5150,\n",
       " 154,\n",
       " 963,\n",
       " 17100,\n",
       " 24862,\n",
       " 12683,\n",
       " 10495,\n",
       " 32150,\n",
       " 222,\n",
       " 7726,\n",
       " 59,\n",
       " 20044,\n",
       " 2895,\n",
       " 7380,\n",
       " 1180,\n",
       " 31563,\n",
       " 4953,\n",
       " 1973,\n",
       " 10073,\n",
       " 4945,\n",
       " 13096,\n",
       " 15149,\n",
       " 15304,\n",
       " 10699,\n",
       " 19947,\n",
       " 9615,\n",
       " 2179,\n",
       " 1465,\n",
       " 9291,\n",
       " 4686,\n",
       " 8514,\n",
       " 2090,\n",
       " 540,\n",
       " 12881,\n",
       " 20252,\n",
       " 16320,\n",
       " 9026,\n",
       " 19094,\n",
       " 25581,\n",
       " 10366,\n",
       " 20243,\n",
       " 1149,\n",
       " 26691,\n",
       " 37,\n",
       " 5654,\n",
       " 8349,\n",
       " 1460,\n",
       " 1682,\n",
       " 1821,\n",
       " 4074,\n",
       " 17229,\n",
       " 10677,\n",
       " 28376,\n",
       " 15569,\n",
       " 12188,\n",
       " 26699,\n",
       " 32869,\n",
       " 6744,\n",
       " 17058,\n",
       " 24515,\n",
       " 713,\n",
       " 15677,\n",
       " 1566,\n",
       " 2991,\n",
       " 24212,\n",
       " 265,\n",
       " 562,\n",
       " 26365,\n",
       " 6303,\n",
       " 435,\n",
       " 19731,\n",
       " 2626,\n",
       " 17708,\n",
       " 8103,\n",
       " 1254,\n",
       " 23130,\n",
       " 9875,\n",
       " 148,\n",
       " 19850,\n",
       " 29023,\n",
       " 438,\n",
       " 13227,\n",
       " 14732,\n",
       " 4378,\n",
       " 9691,\n",
       " 3072,\n",
       " 26543,\n",
       " 9993,\n",
       " 2274,\n",
       " 6303,\n",
       " 1339,\n",
       " 23987,\n",
       " 20704,\n",
       " 10787,\n",
       " 22,\n",
       " 6137,\n",
       " 10556,\n",
       " 48,\n",
       " 16511,\n",
       " 6391,\n",
       " 239,\n",
       " 27179,\n",
       " 26096,\n",
       " 22531,\n",
       " 155,\n",
       " 29554,\n",
       " 21741,\n",
       " 1570,\n",
       " 28589,\n",
       " 568,\n",
       " 10950,\n",
       " 14952,\n",
       " 1398,\n",
       " 16664,\n",
       " 600,\n",
       " 8962,\n",
       " 2314,\n",
       " 2540,\n",
       " 10031,\n",
       " 14524,\n",
       " 26836,\n",
       " 21357,\n",
       " 13832,\n",
       " 3721,\n",
       " 25183,\n",
       " 5500,\n",
       " 25707,\n",
       " 7217,\n",
       " 186,\n",
       " 9256,\n",
       " 99,\n",
       " 11554,\n",
       " 5988,\n",
       " 12176,\n",
       " 3249,\n",
       " 175,\n",
       " 452,\n",
       " 18243,\n",
       " 155,\n",
       " 17547,\n",
       " 11961,\n",
       " 1423,\n",
       " 13003,\n",
       " 5753,\n",
       " 12488,\n",
       " 9727,\n",
       " 4335,\n",
       " 8361,\n",
       " 30396,\n",
       " 4649,\n",
       " 16109,\n",
       " 2817,\n",
       " 1103,\n",
       " 18634,\n",
       " 474,\n",
       " 8309,\n",
       " 10692,\n",
       " 21758,\n",
       " 26436,\n",
       " 1000,\n",
       " 28802,\n",
       " 9142,\n",
       " 20641,\n",
       " 29664,\n",
       " 19701,\n",
       " 98,\n",
       " 25005,\n",
       " 12376,\n",
       " 15550,\n",
       " 91,\n",
       " 33961,\n",
       " 68,\n",
       " 30105,\n",
       " 23667,\n",
       " 13567,\n",
       " 893,\n",
       " 26766,\n",
       " 3608,\n",
       " 8128,\n",
       " 1590,\n",
       " 24612,\n",
       " 4968,\n",
       " 15249,\n",
       " 1185,\n",
       " 367,\n",
       " 3354,\n",
       " 20069,\n",
       " 14040,\n",
       " 5898,\n",
       " 709,\n",
       " 19939,\n",
       " 33234,\n",
       " 13841,\n",
       " 19988,\n",
       " 16999,\n",
       " 7600,\n",
       " 9087,\n",
       " 24187,\n",
       " 28232,\n",
       " 740,\n",
       " 10248,\n",
       " 33489,\n",
       " 24,\n",
       " 16246,\n",
       " 1492,\n",
       " 19165,\n",
       " 7813,\n",
       " 1956,\n",
       " 1695,\n",
       " 6,\n",
       " 13488,\n",
       " 3336,\n",
       " 21843,\n",
       " 11659,\n",
       " 26805,\n",
       " 39,\n",
       " 15546,\n",
       " 2853,\n",
       " 573,\n",
       " 6268,\n",
       " 13027,\n",
       " 1312,\n",
       " 9009,\n",
       " 18500,\n",
       " 716,\n",
       " 25323,\n",
       " 14678,\n",
       " 6159,\n",
       " 26672,\n",
       " 31260,\n",
       " 16241,\n",
       " 19291,\n",
       " 262,\n",
       " 3083,\n",
       " 16089,\n",
       " 2285,\n",
       " 5240,\n",
       " 14077,\n",
       " 339,\n",
       " 8916,\n",
       " 3731,\n",
       " 7511,\n",
       " 4345,\n",
       " 18416,\n",
       " 770,\n",
       " 31303,\n",
       " 17913,\n",
       " 3586,\n",
       " 28152,\n",
       " 17150,\n",
       " 15649,\n",
       " 1492,\n",
       " 4313,\n",
       " 30271,\n",
       " 3039,\n",
       " 366,\n",
       " 1286,\n",
       " 73,\n",
       " 19150,\n",
       " 16666,\n",
       " 4250,\n",
       " 366,\n",
       " 10947,\n",
       " 3881,\n",
       " 17540,\n",
       " 2177,\n",
       " 19841,\n",
       " 2892,\n",
       " 29853,\n",
       " 10344,\n",
       " 2115,\n",
       " 7759,\n",
       " 21548,\n",
       " 10305,\n",
       " 10137,\n",
       " 2722,\n",
       " 7862,\n",
       " 2393,\n",
       " 833,\n",
       " 135,\n",
       " 4056,\n",
       " 22558,\n",
       " 1232,\n",
       " 6270,\n",
       " 5274,\n",
       " 1134,\n",
       " 1638,\n",
       " 941,\n",
       " 8775,\n",
       " 21050,\n",
       " 21313,\n",
       " 2945,\n",
       " 0,\n",
       " 16493,\n",
       " 12510,\n",
       " 12276,\n",
       " 2403,\n",
       " 6212,\n",
       " 11365,\n",
       " 10247,\n",
       " 31365,\n",
       " 19707,\n",
       " 25226,\n",
       " 17086,\n",
       " 25273,\n",
       " 23052,\n",
       " 14831,\n",
       " 2812,\n",
       " 11000,\n",
       " 1198,\n",
       " 975,\n",
       " 8563,\n",
       " 18921,\n",
       " 8460,\n",
       " 10761,\n",
       " 2376,\n",
       " 17289,\n",
       " 23798,\n",
       " 3802,\n",
       " 2352,\n",
       " 157,\n",
       " 3829,\n",
       " 719,\n",
       " 501,\n",
       " 1032,\n",
       " 7635,\n",
       " 438,\n",
       " 1718,\n",
       " 1846,\n",
       " 12887,\n",
       " 29690,\n",
       " 15109,\n",
       " 22904,\n",
       " 26573,\n",
       " 11719,\n",
       " 13884,\n",
       " 27588,\n",
       " 5147,\n",
       " 6887,\n",
       " 32074,\n",
       " 11120,\n",
       " 10,\n",
       " 13348,\n",
       " 2007,\n",
       " 20358,\n",
       " 62,\n",
       " 16282,\n",
       " 892,\n",
       " 6438,\n",
       " 15920,\n",
       " 30825,\n",
       " 1994,\n",
       " 128,\n",
       " 1487,\n",
       " 8326,\n",
       " 1624,\n",
       " 10714,\n",
       " 10883,\n",
       " 2839,\n",
       " 729,\n",
       " 3760,\n",
       " 17641,\n",
       " 3828,\n",
       " 27310,\n",
       " 2140,\n",
       " 4109,\n",
       " 3058,\n",
       " 573,\n",
       " 205,\n",
       " 8074,\n",
       " 12397,\n",
       " 2882,\n",
       " 4092,\n",
       " 7129,\n",
       " 88,\n",
       " 9550,\n",
       " 3199,\n",
       " 260,\n",
       " 4976,\n",
       " 4691,\n",
       " 2452,\n",
       " 845,\n",
       " 12169,\n",
       " 639,\n",
       " 5479,\n",
       " 3344,\n",
       " 1699,\n",
       " 3092,\n",
       " 2927,\n",
       " 834,\n",
       " 1629,\n",
       " 3911,\n",
       " 8247,\n",
       " 5036,\n",
       " 733,\n",
       " 1057,\n",
       " 2513,\n",
       " 1778,\n",
       " 4567,\n",
       " 834,\n",
       " 29627,\n",
       " 6010,\n",
       " 3173,\n",
       " 22,\n",
       " 4795,\n",
       " 7240,\n",
       " 3473,\n",
       " 11811,\n",
       " 5324,\n",
       " 1327,\n",
       " 4873,\n",
       " 20757,\n",
       " 28520,\n",
       " 26710,\n",
       " 2273,\n",
       " 49,\n",
       " 63,\n",
       " 21074,\n",
       " 1961,\n",
       " 8938,\n",
       " 1906,\n",
       " 2390,\n",
       " 520,\n",
       " 4970,\n",
       " 5812,\n",
       " 10455,\n",
       " 349,\n",
       " 12701,\n",
       " 6986,\n",
       " 22648,\n",
       " 1532,\n",
       " 28885,\n",
       " 19586,\n",
       " 8199,\n",
       " 368,\n",
       " 9682,\n",
       " 216,\n",
       " 21847,\n",
       " 4963,\n",
       " 17613,\n",
       " 1300,\n",
       " 25262,\n",
       " 18422,\n",
       " 14849,\n",
       " 281,\n",
       " 547,\n",
       " 21748,\n",
       " 19616,\n",
       " 5173,\n",
       " 13788,\n",
       " 10070,\n",
       " 31365,\n",
       " 15019,\n",
       " 3180,\n",
       " 13234,\n",
       " 7582,\n",
       " 15249,\n",
       " 28932,\n",
       " 4523,\n",
       " 100,\n",
       " 4799,\n",
       " 2970,\n",
       " 3420,\n",
       " 336,\n",
       " 14719,\n",
       " 137,\n",
       " 1676,\n",
       " 144,\n",
       " 24111,\n",
       " 31819,\n",
       " 24641,\n",
       " 3901,\n",
       " 19220,\n",
       " 12853,\n",
       " 2739,\n",
       " 3188,\n",
       " 20955,\n",
       " 16864,\n",
       " 1179,\n",
       " 988,\n",
       " 15990,\n",
       " 14823,\n",
       " 3880,\n",
       " 21655,\n",
       " 381,\n",
       " 28778,\n",
       " 30136,\n",
       " 22240,\n",
       " 20771,\n",
       " 7261,\n",
       " 151,\n",
       " 137,\n",
       " 17646,\n",
       " 919,\n",
       " 21532,\n",
       " 24438,\n",
       " 114,\n",
       " 1043,\n",
       " 825,\n",
       " 2847,\n",
       " 2548,\n",
       " 4746,\n",
       " 15995,\n",
       " 325,\n",
       " 21235,\n",
       " 14392,\n",
       " 11749,\n",
       " 27821,\n",
       " 23480,\n",
       " 2567,\n",
       " 2451,\n",
       " 283,\n",
       " 9064,\n",
       " 12950,\n",
       " 25416,\n",
       " 1592,\n",
       " 1533,\n",
       " 11447,\n",
       " 605,\n",
       " 3729,\n",
       " 20932,\n",
       " 10804,\n",
       " 10493,\n",
       " 13409,\n",
       " 3991,\n",
       " 10469,\n",
       " 24428,\n",
       " 6013,\n",
       " 15157,\n",
       " 1364,\n",
       " 18037,\n",
       " 26857,\n",
       " 6940,\n",
       " 31246,\n",
       " 18811,\n",
       " 10093,\n",
       " 29759,\n",
       " 2012,\n",
       " 13208,\n",
       " 16814,\n",
       " 4333,\n",
       " 620,\n",
       " 16792,\n",
       " 30440,\n",
       " 33753,\n",
       " 29251,\n",
       " 3023,\n",
       " 32134,\n",
       " 20805,\n",
       " 0,\n",
       " 1352,\n",
       " 12046,\n",
       " 14,\n",
       " 38,\n",
       " 4964,\n",
       " 401,\n",
       " 24698,\n",
       " 1064,\n",
       " 31267,\n",
       " 2725,\n",
       " 16257,\n",
       " 495,\n",
       " 2716,\n",
       " 18830,\n",
       " 1659,\n",
       " 51,\n",
       " 16364,\n",
       " 6281,\n",
       " 3937,\n",
       " 19926,\n",
       " 13543,\n",
       " 1498,\n",
       " 797,\n",
       " 26755,\n",
       " 8960,\n",
       " 32181,\n",
       " 30932,\n",
       " 33381]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Split  sentences into words in order to build vocab and make mapping between words and integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "\n",
    "def vectorize_text(target_text, other_text):\n",
    "    target_sentence_list = split_into_sentences(target_text)\n",
    "    other_sentence_list = split_into_sentences(other_text)\n",
    "    print('num of target, other sentences', len(target_sentence_list), len(other_sentence_list))\n",
    "    \n",
    "    word_tokenizer = TreebankWordTokenizer().tokenize\n",
    "    word_set = set()\n",
    "    for sentence in target_sentence_list+other_sentence_list:\n",
    "        word_set = word_set.union(set(word_tokenizer(sentence)))\n",
    "    print('number of words in vocab', len(word_set))\n",
    "    word_list = list(word_set)\n",
    "    num_to_word = {j:word for j, word in enumerate(word_list)}\n",
    "    word_to_num = {word:j for j, word in enumerate(word_list)}\n",
    "    \n",
    "    target_vectorized_sentences = np.array([np.array([word_to_num[word] for \\\n",
    "                                               word in word_tokenizer(sentence)]) \\\n",
    "                                            for sentence in target_sentence_list])\n",
    "    \n",
    "    other_vectorized_sentences = np.array([np.array([word_to_num[word] for \\\n",
    "                                               word in word_tokenizer(sentence)]) \\\n",
    "                                            for sentence in other_sentence_list])\n",
    "    \n",
    "    return target_vectorized_sentences, other_vectorized_sentences, num_to_word, word_to_num, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of target, other sentences 35012 547017\n"
     ]
    }
   ],
   "source": [
    "target_vectorized_sentences, other_vectorized_sentences, num_to_word, word_to_num, word_list \\\n",
    "    = vectorize_text(austen_text, other_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_subsets(vect):\n",
    "    train_indices = np.random.choice(len(vect), len(vect)//2, replace=False)\n",
    "    test_indices = [i for i in range(len(vect)) if i not in train_indices]\n",
    "    result_test = vect[test_indices]\n",
    "    result_train = vect[train_indices]\n",
    "    return result_test, result_train\n",
    "\n",
    "\n",
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "\n",
    "def make_test_train(targ_vect_sent, other_vect_sent):\n",
    "    targ_test, targ_train = make_subsets(targ_vect_sent)\n",
    "    other_test, other_train = make_subsets(other_vect_sent)\n",
    "    X_train = np.concatenate((targ_train, other_train))\n",
    "    X_test = np.concatenate((targ_test, other_test))\n",
    "    y_train = np.array([1]*len(targ_train)+ [0]*len(other_train))\n",
    "    y_test = np.array([1]*len(targ_test) + [0]*len(other_test))\n",
    "    X_train, y_train = shuffle_in_unison(X_train, y_train)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = make_test_train(target_vectorized_sentences, other_vectorized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for  RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austenbooks\timdb.ipynb     Untitled1.ipynb\tX_test.pkl\r\n",
      "Austen.ipynb\tmoby_dick.txt  Untitled.ipynb\tX_train.pkl\r\n",
      "get_text.ipynb\tOANC-GrAF      untitled.txt\ty_train.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('X_train.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(X_train, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('y_test.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(y_test, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austenbooks\timdb.ipynb\t Untitled.ipynb  X_train.pkl\r\n",
      "Austen.ipynb\tmoby_dick.txt\t untitled.txt\t y_test.pkl\r\n",
      "get_text.ipynb\tUntitled1.ipynb  X_test.pkl\t y_train.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 500, 32)           8764672   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 8,847,233.0\n",
      "Trainable params: 8,847,233\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/keras/models.py:826: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 291014 samples, validate on 291015 samples\n",
      "Epoch 1/5\n",
      " 70400/291014 [======>.......................] - ETA: 8083s - loss: 0.1377 - acc: 0.9579"
     ]
    }
   ],
   "source": [
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_list), embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "l=[]\n",
    "l.append(model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=5, batch_size=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': [0.77744712509299607, 0.94590285896482096],\n",
       " 'loss': [0.45427668586853193, 0.14299210789239769],\n",
       " 'val_acc': [0.86408076514346444, 0.93421891604675877],\n",
       " 'val_loss': [0.27790165316238286, 0.15308598510037186]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_prob_on_sentence(sentence, model):\n",
    "    word_tokenizer = TreebankWordTokenizer().tokenize\n",
    "    vec_sentence = np.array([np.array([word_to_num[word] for \\\n",
    "                                               word in word_tokenizer(sentence)])])\n",
    "    padded_sentence = sequence.pad_sequences(vec_sentence, maxlen=max_review_length)\n",
    "    return model.predict_proba(padded_sentence)\n",
    "\n",
    "def predict_prob_on_paragraph(paragraph, model):\n",
    "    sentence_list = split_into_sentences(paragraph)\n",
    "    results = []\n",
    "    for sentence in sentence_list:\n",
    "        results.append(predict_prob_on_sentence(sentence, model)[0][0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paragraph = \"\"\"“I do not understand what you mean by ‘success,’” said Mr. Knightley. “Success supposes endeavour. Your time has been properly and delicately spent, if you have been endeavouring for the last four years to bring about this marriage. A worthy employment for a young lady’s mind! But if, which I rather imagine, your making the match, as you call it, means only your planning it, your saying to yourself one idle day, ‘I think it would be a very good thing for Miss Taylor if Mr. Weston were to marry her,’ and saying it again to yourself every now and then afterwards, why do you talk of success? Where is your merit? What are you proud of? You made a lucky guess; and that is all that can be said.”\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.98919564,\n",
       " 0.50044167,\n",
       " 0.84341258,\n",
       " 0.81804514,\n",
       " 0.96195269,\n",
       " 0.37761614,\n",
       " 0.47974524,\n",
       " 0.56973124]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_prob_on_paragraph(paragraph, model)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
